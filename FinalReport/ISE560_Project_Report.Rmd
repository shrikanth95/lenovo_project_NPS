---
title: "ISE 560 Project Report"
author: "Shawn Markham, Melissa Wong, Shrikanth Yadav & Lekhana Yennam"
date: \today
header-includes:
   - \usepackage{fontspec}
   - \setmainfont{Times New Roman}
   - \usepackage{setspace}
   - \onehalfspacing
   - \usepackage{enumitem}
   - \usepackage{float}
   - \usepackage{url}
   - \usepackage{listings}
   # - \usepackage{biblatex}
   # - \addbibresource{ise560report.bib}
   - \pagenumbering{gobble}
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    fig_caption: true
documentclass: article
fontsize: 12pt
geometry: margin=1in 
urlcolor: blue
---

\begin{figure}[b!]
  \centering
  \includegraphics[width = 0.48\textwidth]{images/ise_logo.pdf}
\end{figure}

\newpage

\pagenumbering{roman}
\setcounter{page}{2}

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage

\pagenumbering{arabic}

\section{Executive Summary}

\section{Introduction}

Lenovo is an international company and a market leader in the personal computer industry.  One tool Lenovo relies upon in assessing customer satisfaction is Net Promoter Score (NPS); this is a standard metric used by major corporations across many industries.  However, latency is a limitation because it can take several months to aggregate sufficient data to calculate NPS.  Therefore our team was tasked with developing a model to predict NPS from more timely information such as customer comments and ratings from websites and telemetry data.

In addition to developing a predictive model for NPS, our team also characterized the natural evolution of both NPS and customer sentiment over time.  These stochastic models form the basis for a Markov Decision Process (MDP) which can be used by Lenovo to identify the optimal actions to take based on the available information.  

Finally, our team analyzed the customer sentiment and comments to identify factors correlated with the highest and lowest satistfaction products.  These results may help identify areas for Lenovo to focus further investments that can improve long-term customer satistifaction. Subsequent sections of this document provide detailed explanations of each phase of our analysis and associated recommendations.

\section{Predictive Model}

Our team considered three approaches to developing a predictive model--linear regression, proportional odds model and time series analysis--because these seemed potentially suitable for this problem and the team had previous experience with these modeling approaches.  We ultimately chose a linear model which will be described in detail here; a summary of the other methods and why we do not recommend them at this time can be found in Appendix \ref{app:PredModel}.

We fit numerous linear models using combinations of the options listed below:
\begin{enumerate}
  \item Epoch: raw data grouped by week, bi-monthly and monthly
  \item Segment: single segment (commercial and consumer combined) versus separate segements
  \item Predictors: Product Sentiment Index (PSI), average star rating, Product Series, Product Name and interactions between predictors
\end{enumerate}

We then evaluated each model in a two-step process.  First, we examined residuals plots of each model to determine if key assumptions (e.g., normality and homogeneity of variance) of a linear model were satisfied.  Aggregating data weekly or bi-monthly often meant there were very few data points per epoch and/or there would be significant outliers which meant that one or both of the key assumptions were not satisfied.  Therefore we determined aggreating data by month yielded beter results for fitting the linear model.  The second phase of our analysis used F-tests/Analysis of Variance (ANOVA) to identify predictors in the full model (PSI, average stars, Series and Series*PSI interaction) which did not significantly improve the model fit and thus could be removed to yield a more parsimonious model.  Detailed steps in the model selection process can also be found in Appendix \ref{app:PredModel}.  Our final recommendation is two different models for the consumer and commercial segments:

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width="75%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(error = FALSE)

library(tidyverse)
library(lubridate)
library(grid)
library(gridExtra)
#library(kableExtra)
library(car)
library(caret)

source("../src_lenovo_project.R")
source("../LenovoAnalysis/data_prep.R")
source("../LenovoAnalysis/helper_calcs.R")
source('../LenovoAnalysis/MDP_lenovo_helper.R', encoding = 'UTF-8')
filter_raw_data()

load("../CleanData/filtered_sentiment_data.Rdata")
load("../CleanData/filtered_survey_data.Rdata")
load("../CleanData/filtered_product_lists.Rdata")
load("../CleanData/timeSeries.Rdata")
```

```{r}
# Consumer Only
psi <- sentiment.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_consumer <- inner_join(psi, nps) %>%
  filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Fit model without interaction term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl2 <- train(nps ~ SeriesName + psi + stars, data=all_consumer,
              method = "lm", trControl = train.control)
```

$$NPS_{consumer} = (2.43 + \beta_1) - 0.16 * PSI + 13.42 * stars$$
$$NPS_{commercial} = (38.87 + \beta_1) + (\beta_2 - 0.49) * PSI$$

where $\beta_i$ is unique for each Series in each segment as shown in the tables below:

```{r}
# Commercial Only
psi <- sentiment.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_comm <- inner_join(psi, nps) %>%
  #filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Commercial model without stars
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl4 <- train(nps ~ SeriesName*psi, data=all_comm,
              method = "lm", trControl = train.control)
```

```{r}
# Consumer
beta_coeff <- round(mdl2$finalModel$coefficients[2:14], digits=2)
nconsumer <- sub("SeriesName", "", names(beta_coeff))
knitr::kable(data.frame(nconsumer, unname(beta_coeff)),
             col.names = c("Consumer Series", "$\\beta_1$"),
             caption = "Coefficients for Consumer Model") 

# Commercial
beta_coeff1 <- round(mdl4$finalModel$coefficients[2:12], digits=2)
beta_coeff2 <- round(mdl4$finalModel$coefficients[14:24], digits=2)
ncomm <- sub("SeriesName", "", names(beta_coeff1))
knitr::kable(data.frame(ncomm, unname(beta_coeff1), unname(beta_coeff2)), 
             col.names = c("Commercial Series", "$\\beta_1$", "$\\beta_2$"),
             caption = "Coefficients for Commercial Model") 

```

TO DO: Figure out how to place tables side by side or if space is tight move to Appendix A

After model selection, we used k-fold cross validation to estimate the accuracy of the model for future predictions. While the model selection process did result in statistically significant coefficients for PSI and stars, the results from the k-fold cross validation show that both model still leave significant room for improvement based on the Root Mean Square Error and Mean Absolute Error (Table \ref{tbl:KFold}).

\begin{table}[ht]
\centering
\caption{Summary of K-fold Cross-Validation Results}
\begin{tabular}{l|c|c|c|}
\cline{2-4}
\multicolumn{1}{c|}{}                  & F-test p-value   & RMSE  & MAE    \\ \hline
\multicolumn{1}{|l|}{Consumer Model}   & \textless{}0.001 & 25.96 & 19.69  \\ \hline
\multicolumn{1}{|l|}{Commercial Model} & \textless{}0.001 & 28.85 & -21.73 \\ \hline
\end{tabular}
\label{tbl:KFold}
\end{table}

It is perhaps easier to understand these results by comparing plots of observed versus predicted.  Figures \ref{fig:mdlConsumer} and \ref{fig:mdlComm} are scatterplots for the Consumer and Commercial data, respectively.  In both sets of plots, the grey points are the observed (NPS, PSI) scores, the grey line is the regression line for the marginal model $NPS = \beta_o + \beta_1 * PSI$ and the red dots are the _predicted values_ from equations (1) and (2) respectively.  One can clearly see that while the predicted (red) points follow the general trend of the observed (grey) points, there is still significant variation that is not accounted for by the models.  If Lenovo started archiving the telemetry data, then new models incorporating the telemetry could be fit once several months of data is available; it is entirely possible incorporating telemetry data into the model could reduce the unaccounted for variation and give better predictions.

TO DO: Number equations for linear models

```{r fig.cap="\\label{fig:mdlConsumer}Marginal Model Plots, Consumer NPS vs. PSI"}
# Look at scatterplots fitted vs actual

cbind(all_consumer, 
      pred2=predict(mdl2$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred2), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

```{r fig.cap="\\label{fig:mdlComm}Marginal Model Plots, Commercial NPS vs. PSI"}
# Look at scatterplots fitted vs actual

cbind(all_comm, 
      pred4=predict(mdl4$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred4), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

\section{Markov Decision Process}

The next phase of our analysis was to develop a Markov decision process (MDP) to identify optimal policies based on customer sentiment or satisfaction.  Where Lenovo did not identify specific componenents (e.g., actions, rewards) necessary for the MDP, we will make assumptions and identify them as such.  As much as possible, we parameterized the assumptions so that Lenovo can easily modify our software to their specifications in the future.

\subsection{States}\label{sec:statedef}

We considered using either PSI or predicted NPS (pNPS) states as the basis for the MDP.  The challenge with using PSI is that choosing states is somewhat arbitrary.  We considered standardizing the PSI scores and then delineating states using multiples of the standard deviation.  However, we felt that approach would not be as intuitive to understand because the states would not be defined according to the [-100, 100] scale Lenovo is familiar with for PSI.  On the other hand, there are established industry-standard categories for NPS\cite{Nps19}.  We chose to use these industry-standards as the basis for the pNPS states in our MDP, and they are listed below.

\begin{itemize}
  \item Very Low: $NPS \le 8$
  \item Moderately Low: $8 < NPS \le 35$
  \item Normal: $35 < NPS \le 45$
  \item Moderately High: $45 < NPS \le 65$
  \item Very High: $65 < NPS$
\end{itemize}

\subsection{Actions}

We assumed two possible actions: {Do Nothing, Conduct Root Cause Analysis}

<!-- \begin{table}[h!] -->
<!-- \begin{tabular}{lc|c|c|c|c|c|} -->
<!-- \cline{3-7} -->
<!-- \multicolumn{1}{c}{}     &                & \multicolumn{5}{c|}{Applicable NPS States}  \\ \hline -->
<!-- \multicolumn{1}{|c|}{ID} & Action         & Very Low & \begin{tabular}[c]{@{}c@{}}Moderately\\ Low\end{tabular} & Normal & \begin{tabular}[c]{@{}c@{}}Moderately \\ High\end{tabular} & Very High \\ \hline -->
<!-- \multicolumn{1}{|l|}{1}  & Do Nothing                  & Y  & Y & N      & N  & N      \\ \hline -->
<!-- \multicolumn{1}{|l|}{2}  & Conduct root cause analysis & Y  & Y & N      & N  & N       \\ \hline -->
<!-- \end{tabular} -->
<!-- \end{table} -->

\begin{table}[h!]
\centering
\caption{Example Actions}
\begin{tabular}{lc|c|c|c|c|c|}
\cline{3-7}
\multicolumn{1}{c}{}     &                & \multicolumn{5}{c|}{Applicable NPS States}  \\ \hline
\multicolumn{1}{|c|}{ID} & Action         & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{1}  & Do Nothing                  & Y  & Y & N      & N  & N      \\ \hline
\multicolumn{1}{|l|}{2}  & Conduct root cause analysis & Y  & Y & N      & N  & N       \\ \hline
\end{tabular}
\end{table}

\subsection{Transitions}

Using the above NPS states, we then calculated the natural evolution of NPS by month (i.e., same epoch as linear model).  This natural evolution is equivalent to the "Do Nothing" Action.

```{r} 

```

\begin{table}[h!]
\caption{Consumer, Action = Do Nothing}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & 0.451    & 0.220    & 0.055   & 0.143 & 0.132    \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & 0202    & 0.506    & 0.079   & 0.101  & 0.111       \\ \hline
\multicolumn{1}{|l|}{Normal}   & 0.161   & 0.355  & 0.194     & 0.160   & 0.097       \\ \hline
\multicolumn{1}{|l|}{Mod High} & 0.200  & 0.220    & 0.160     & 0.200  & 0.220       \\ \hline
\multicolumn{1}{|l|}{Very High} & 0.214    & 0.054    & 0.125     & 0.161    & 0.446      \\ \hline
\end{tabular}
\label{tbl:P1}
\end{table}

Next, we assume the action "Conduct root cause analysis" reduces the probability of transitioning to Very_Low by $p\%$ as compared to "Do Nothing"", and that percentage is evenly distributed among the remaining 4 states.  For example, if $p=10\%$ then the transition matrix for "Conducting root cause analysis" is shown in Table \ref{tbl:P2}.


\begin{table}[h!]
\caption{Consumer, Action = Conduct Root Cause Analysis}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & 0.406    & 0.231    & 0.066   & 0.154 & 0.143    \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & 0.182    & 0.511    & 0.084   & 0.106  & 0.117       \\ \hline
\multicolumn{1}{|l|}{Normal}   & 0.161   & 0.355  & 0.194     & 0.160   & 0.097       \\ \hline
\multicolumn{1}{|l|}{Mod High} & 0.200  & 0.220    & 0.160     & 0.200  & 0.220       \\ \hline
\multicolumn{1}{|l|}{Very High} & 0.214    & 0.054    & 0.125     & 0.161    & 0.446      \\ \hline
\end{tabular}
\label{tbl:P2}
\end{table}

\subsection{Rewards}

We will assume Rewards are of the form shown in Table \ref{tbl:Reward1} with a parameter $r$.

\begin{table}[h!]
\caption{Reward Matrix Template}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & -2r      & -r       & 0      & r         & 2r        \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & -2r      & -r       & 0      & r         & 2r        \\ \hline
\multicolumn{1}{|l|}{Normal}   & -2r      & -r      & 0      & r          & 2r        \\ \hline
\multicolumn{1}{|l|}{Mod High} & -2r      & -r      & 0      & r          & 2r        \\ \hline
\multicolumn{1}{|l|}{Very High} & -2r      & -r     & 0      & r          & 2r        \\ \hline
\end{tabular}
\label{tbl:Reward1}
\end{table}

Next, we assume $r=10$ and conducting a root cause analysis has cost $c=5$.  The resulting Rewards Matrices are shown in Tables \ref{tbl:Reward2} and \ref{tbl:Reward3}.

\begin{table}[h!]
\caption{Example Reward Matrix- Do Nothing}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}     & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & -20      & -10       & 0      & 10         & 20        \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & -20      & -10       & 0      & 10         & 20        \\ \hline
\multicolumn{1}{|l|}{Normal}   & -20      & -10      & 0      & 10          & 20        \\ \hline
\multicolumn{1}{|l|}{Mod High} & -20      & -10      & 0      & 10          & 20        \\ \hline
\multicolumn{1}{|l|}{Very High} & -20      & -10     & 0      & 10          & 20        \\ \hline
\end{tabular}
\label{tbl:Reward2}
\end{table}

\begin{table}[h!]
\caption{Example Reward Matrix- Conduct Root Cause Analysis}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & -25      & -15       & -5      & 5         & 15        \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & -25      & -15       & -5      & 5         & 15        \\ \hline
\multicolumn{1}{|l|}{Normal}   & -      & -      & -      & -          & -        \\ \hline
\multicolumn{1}{|l|}{Mod High} & -      & -      & -      & -          & -        \\ \hline
\multicolumn{1}{|l|}{Very High} & -      & -     & -      & -          & -        \\ \hline
\end{tabular}
\label{tbl:Reward3}
\end{table}

\subsection{Future MDP}

We present a framework to use temeletry and sentiment to make decisions on the releasing software interventions using Markov Decision Processes.  In what follows, we describe the states, actions and compute transition probabilities with intervention.

## States

We consider the combination of telemetry and user survey as our states. We present a detailed description of the states and ranges.
<!-- $\{(++,N), (+,N),(0,N),(-,N),(- -,N),(++,AN), (+,AN),(0,AN),(-,AN),(- -,AN)\}$.   -->

### Survey Observations

NPS values are charasterized based on the industry considerations given in Section \ref{sec:statedef} denoted as $S_S \in \{++,+, 0, -, - -\}$ for \{ Very high, Moderately high, Normal, Moderately low, Very low\} respectively.  The transition probabilities are denoted as $P_{S}$ and is as given in Table.\ref{tab:consumer_dn}.

```{r Preparation of sentiment matrix, echo=FALSE}

n.sent = 5
tmp <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  calcNPS()
# Define PSI categories
VERY_HIGH <- 60.0
MOD_HIGH <- 45.0
NORMAL <- 35.0
MOD_LOW <- 8.0

res <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  NPSTransitions(VERY_HIGH, MOD_HIGH, NORMAL, MOD_LOW)


tmp = as.matrix(res$P,ncol = 5,nrow = 5)
p.s = matrix(as.numeric(tmp[1:5,2:6]),ncol = 5, nrow= 5) %>% round(digits = 3)
rownames(p.s) = colnames(tmp)[2:6]
colnames(p.s) = colnames(tmp)[2:6]
# knitr::kable(res$P, row.names=TRUE, caption = "Transition matrix for consumer surveys (Monthly)")

```

### Telemetry

Assuming a time-series of driver health is available for a particular driver, states can be defined between normal $N$ and abnormal $AN$ operations depending on a performance score $ c_t $ at time $t$ and is given in Equation.\ref{crash_rate_score}.
\begin{equation}\label{crash_rate_score}
c_{t} = min\left\{1,\frac{\text{\# Crashes}}{\text{Total machines }\times \text{ Percent Impacted}}\right\}
\end{equation}
Note that our results would be more relevant if we have access to the number of unique machines that have experienced a crash.  This is can defined as 
$$c_t = \frac{\text{ Number of unique machines crashing at time $t$ }}{\text{Total Number of Machines Impacted at time $t$}}$$

When the performance score is above a threshold, we flag the observations as Anomalous. The transition matrix for telemetry can be constructed similar to the pNPS transition matrix by observing the number of transitions from each state for a given driver update.   specific driver updates (Two states $S_T \in \{N, AN\}$) and is referred to as $P_{T}$.  For the sake of demonstration, we assume the following transition matrix for telemetry:

```{r, echo=FALSE, eval=TRUE}

n.tel = 2
p.tel = matrix(c(0.75, 0.25, 0.25, 0.75),nrow = 2, ncol = 2, byrow = TRUE)
rownames(p.tel) <- c("N","AN")
colnames(p.tel) <- c("N","AN")
knitr::kable(p.tel ,  caption = "A possible telemetry transtion matrix from ",  row.names = TRUE)

```

The final state matrix $P$ would be a combination of $P_S$ and $P_T$ given by

$$P_{1}(s,t->s_1,t_1) = P_S(s)P_S(s_1)P_T(t)P_T(t_1)$$

```{r eval=TRUE}
# Calcualtion of combined state probability matrix
P1 = combine.telemetry.survey(number.survey.states = n.sent,
                            number.telemetry.states = n.tel,
                            survey.probability.matix = p.s,
                            telemetry.probability.matix = p.tel)
P1 <- round(P1,digits = 3)
rownames(P1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
colnames(P1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
knitr::kable(P1 ,  caption = "A combined Telemetry-Sentiment State transition matrix.", row.names = TRUE)


```


## Actions

The actions are chosen between performing an intervention and doing nothing.

- A shift towards normal operating mode is expected in the telemetry state transition matrix associated with an intervention.  The transition matrix is denoted as $P_{T,1}$.  As the telemetry is not provided, we assume that the transition probabilities corresponding to positive and negative sentiment will increase and decrease respectively by a predefined factor $\gamma_T$.  Note that $\gamma_T$ can be estimated by observing the percentage of impacted machines due to an intervention.

- A relatively small shift is expected in the sentiment transition matrix as the population is relatively late to react to improvements.  This is referred to as $P_{S,1}$.The new sentiment transition matrix can be estimated similarly by choosing the fraction by which probabilities will change.  This is denoted by $\gamma_S$. Note that $\gamma_S<\gamma_T$.


The joint transition matrix $P_1$ is associated with the combination of $P_{S,1}$ and $P_{T,1}$.

## Rewards

A cost $r_k$ is incurred when an intervention $k\in\{1,2\}$ is performed.  The no-action cost matrix that incentivize transitions towards higher sentiment scores are denoted by $R_{T,S,1}$.

```{r, echo= FALSE}
R.st1 = matrix(c(140, 120, 75, 50, 30, -10, -25, -50, -100, -120,
                 130, 110, 70, 40, 20, -15, -30, -55, -100, -120,
                 120, 100, 65, 30, 10, -25, -35, -60, -100, -120,
                 120, 100, 60, 20, 5, -35, -40, -65, -100, -120,
                 120, 100, 55, 10, 0, -40 ,-45, -70, -100, -120,
                 120, 100, 70, 45, 30, -45 ,-50, -75, -100, -120,
                 120, 100, 65, 40, 20, -50, -55, -80, -100, -120,
                 120, 100, 60, 35, 10, -55, -60, -85, -100, -120,
                 120, 100, 55, 30, 5, -60, -65, -90, -110, -130,
                 120, 100, 50, 25, 0, -65, -70, -95, -120, -140),nrow = n.sent*n.tel, ncol = n.sent*n.tel,byrow = TRUE)
rownames(R.st1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
colnames(R.st1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
knitr::kable(R.st1 ,  caption = "A suggested no-action reward function",  row.names = TRUE)
```

The reward matrix $R_{S,T,2}$ associated with an intervention should be calculated based on $P_{1}$ as $$R_{S,T,2}(s,t->s_1,t_1) = -r_2+\sum_{x}\left(R_{S,T,1}(s,t->x)\right) P_1(s,t->x)$$ where $x\in\{(+,N),(0,N),(-,N),(+,AN),(0,AN),(-,AN)\}$

\section{Customer Sentiment Analysis}

\section{Recommendations}

\clearpage

\bibliographystyle{unsrt}
\bibliography{ise560report}
<!-- \printbibliography -->

\clearpage

\appendix

\section{Predictive Model Approaches}\label{app:PredModel}

\subsection{Linear Model}

\subsection{Proportional Odds Model}

\subsection{Time Series Analysis}

Considering the pNPS score as a stochastic process where the random variables are realizations of pNPS value, attempts to perform classical  time-series analysis.  If $X_t$ denoted pNPS at time $t$, the auto-regressive moving-average(ARMA) model \cite{box2015time} would be

$$X_{t} = \sum_{i = 1}^p \alpha_i X_{t-i} + \sum_{j = 1}^q  \beta_j \theta_{t-j} $$.

where $\alpha_i$ $\forall i \in \{1,2,\ldots, p\}$  and $\beta_j$ $\forall j \in \{1,2,\ldots, q\}$ are coefficients of the linear model making a 1-step prediction of pNPS,  $\theta_t$ is the moving average component at time $t$ following a normal distribution with zero mean and variance $\sigma^2_w$. As the pNPS scores are heteroscedastic, the time series are also differenced $d$ times to satisfy the condition of stationarity.  Note that $p$, $d$ and $q$ are  referred to as the order of the model.  In Tables \ref{tab:ARIMA_Commercial} and \ref{tab:ARIMA_Consumer}, we present train set errors of best ARIMA model with the lowest Akaike information criterion (AIC) with order $(p,q,r)$.  The pNPS calcualtions were made over a period of seven days.  It is also observed that temporal correlation is sensitive to the series.

```{r}
series.ts.main.com <- series.ts.main.com%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0,SeriesName != "ACCESSORIES", SeriesName != "ANNIVERSARY")
series.ts.main <- series.ts.main%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0, SeriesName != "SMART HOME", SeriesName != "OTHER")
knitr::kable(series.ts.main.com[series.ts.main.com$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Commercial", caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))
knitr::kable(series.ts.main[series.ts.main$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Consumer",caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))


```


While the intention is analyze the statistical relationship between adjacent samples of pNPS calculations, the epoch overwhich the pNPS scores are varied from 1 to 30 days.  Figures \ref{fig:arima.comm} and \ref{fig:arima.con} shows the Mean Absolute Errors as a funtion of the epoch over which the pNPS score was calculated.  It is observed for a majority of the Series, the surveys are least correlated with a two-week window.

```{r figs, echo = FALSE,  fig.cap = "\\label{fig:arima.comm} Train set pNPS prediction errors vs window size for commercial data.", fig.width = 8}
plt.com <- ggplot(series.ts.main.com, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.com
```

```{r figs_con, echo = FALSE, fig.cap="\\label{fig:arima.con} Train set pNPS prediction errors vs window size for consumer data.", fig.width = 8}
plt.con <- ggplot(series.ts.main, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.con
```

\clearpage

\section{Markov Decision Process}


```{r echo=FALSE}
rm(list=ls())

library(tidyverse)
library(lubridate)
library(grid)
library(gridExtra)
library(MDPtoolbox)

source("../src_lenovo_project.R")
source("../LenovoAnalysis/data_prep.R")
source("../LenovoAnalysis/helper_calcs.R")
source('../LenovoAnalysis/MDP_lenovo_helper.R', encoding = 'UTF-8')
filter_raw_data()

load("../CleanData/filtered_sentiment_data.Rdata")
load("../CleanData/filtered_survey_data.Rdata")
load("../CleanData/filtered_product_lists.Rdata")
load("../CleanData/timeSeries.Rdata")

# Define NPS states
NPS_VERY_HIGH <- 60.0
NPS_MOD_HIGH <- 45.0
NPS_NORMAL <- 35.0
NPS_MOD_LOW <- 8.0

res <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  NPSTransitions(NPS_VERY_HIGH, NPS_MOD_HIGH, NPS_NORMAL, NPS_MOD_LOW)

PConsumer_A1 <- res$P

#knitr::kable(PConsumer_A1, row.names=TRUE, caption = "Consumer, Action=Do Nothing")

p <- 0.1
n <- nrow(PConsumer_A1)

# Modify VERY_LOW
PConsumer_A2 <- PConsumer_A1
delta <- p * PConsumer_A1[1,2]
PConsumer_A2[1,2] <- PConsumer_A2[1,2] - delta
PConsumer_A2[1,3] <- PConsumer_A2[1,3] + delta/(n-1)
PConsumer_A2[1,4] <- PConsumer_A2[1,4] + delta/(n-1)
PConsumer_A2[1,5] <- PConsumer_A2[1,5] + delta/(n-1)
PConsumer_A2[1,6] <- PConsumer_A2[1,6] + delta/(n-1)

# Modify MODERATELY_LOW
delta <- p * PConsumer_A1[2,2]
PConsumer_A2[2,2] <- PConsumer_A2[2,2] - delta
PConsumer_A2[2,3] <- PConsumer_A2[2,3] + delta/(n-1)
PConsumer_A2[2,4] <- PConsumer_A2[2,4] + delta/(n-1)
PConsumer_A2[2,5] <- PConsumer_A2[2,5] + delta/(n-1)
PConsumer_A2[2,6] <- PConsumer_A2[2,6] + delta/(n-1)

#knitr::kable(PConsumer_A2, row.names=TRUE, caption = "Consumer, Action=Root Cause Analysis")

```


\clearpage

\section{Additional R code}

\subsection{data\_prep.R}

\lstinputlisting{../LenovoAnalysis/data_prep.R}

\subsection{helper\_calcs.R}

\lstinputlisting{../LenovoAnalysis/helper_calcs.R}
