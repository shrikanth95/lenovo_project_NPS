---
title: "ISE 560 Project Report"
author: "Shawn Markham, Melissa Wong, Shrikanth Yadav & Lekhana Yennam"
date: \today
header-includes:
   - \usepackage{fontspec}
   - \setmainfont{Times New Roman}
   - \usepackage{setspace}
   - \onehalfspacing
   - \usepackage{enumitem}
   - \usepackage{float}
   - \usepackage{subfig}
   - \usepackage{url}
   - \usepackage{listings}
   # - \usepackage{biblatex}
   # - \addbibresource{ise560report.bib}
   - \pagenumbering{gobble}
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    fig_caption: true
documentclass: article
fontsize: 12pt
geometry: margin=1in 
urlcolor: blue
---

\begin{figure}[b!]
  \centering
  \includegraphics[width = 0.48\textwidth]{images/ise_logo.pdf}
\end{figure}

\newpage

\pagenumbering{roman}
\setcounter{page}{2}

\tableofcontents
\newpage

\listoffigures

\listoftables

\clearpage

\section*{Executive Summary}
\setcounter{page}{5}
\addcontentsline{toc}{section}{Executive Summary}

Our team was tasked with investigating methods for predicting Net Promoter Score (NPS) using customer sentiment, star rating and telemetry data.  First, we considered several approaches to developing a predictive model using various combinations of those predictors.  We ultimately determined two different linear models, one for the Consumer Segment and one for the Commercial Segment, resulted in the best predictions of the candidate approaches.  For the Consumer segment, we determined that the Series, Product Sentiment Index (PSI) and average star ratings were the most useful predictors.  For the Commercial Segment, Series and PSI (but not average star rating) were the most useful predictors.  Because Lenovo was able to provide only a small amount of telemetry data, we were not able to directly incorporate the information into the models.  However, even though there was a statistically significant relationship between NPS and the predictors previously mentioned, there is still significant variation in the NPS scores that are not accounted for by the model.  In other words, PSI, Series and average star rating only partially explain the observed NPS scores. We hypothesize that if Lenovo can build up an archive of the telemetry data (e.g., 6-12 months worth) which is then incorporated into the model, that may help reduce the amount of unexplained variation in the NPS scores.

Next, we developed a framework for a Markov Decision Process (MDP) based on the predicted NPS score and categories of Normal vs. Abnormal telemetry activity (assuming that Lenovo will begin archiving the telemetry data).  We had to make many assumptions (e.g., potential actions Lenovo might take, the rewards/costs associated with those actions) in order to formulate the MDP.  Given all of these assumptions, the _specific_ results of the MDP are of less importance to Lenovo than the analysis process and the framework.  We have provided all of the source code used to build and analyze the MDP in the Appendices, and all of the assumptions are parameters in the source code.  Our goal was to deliver a framework and tools that Lenovo could easily modify for future use. 

Finally, we identified the three products with the lowest average NPS score in each business segment.  The bottom three Consumer products are the Ideapad 110, the Yoga 710 and the Ideapad 120S; the bottom three Commercial products are the E585, E485 and E580.  Then we analyzed the customer sentiment data to identify the categories of customer sentiment with the strongest correlation to PSI for each of the lowest performing products.  Although the relevant categories differ among the six categories, there are a few categories which are common to more than one product.  These common categories include Battery, Memory and Display.  While we cannot conclude improvements in these categories will improve PSI and NPS scores based on the data available (i.e., correlation does not imply causation), the strong correlation does justify Lenovo considering these categories as areas for potential improvement.

\clearpage

\pagenumbering{arabic}

\section{Introduction}

Lenovo is an international company and a market leader in the personal computer industry.  One tool Lenovo relies upon in assessing customer satisfaction is Net Promoter Score (NPS); this is a standard metric used by major corporations across many industries.  However, latency is a limitation because it can take several months to aggregate sufficient data to calculate NPS.  Therefore our team was tasked with developing a model to predict NPS from more timely information such as customer comments and ratings from websites and telemetry data.

In addition to developing a predictive model for NPS, our team also characterized the natural evolution of NPS over time.  This stochastic model forms the basis for a Markov Decision Process (MDP) which can be used by Lenovo to identify the optimal actions to take based on available information.  

Finally, our team analyzed the customer sentiment and comments to identify factors correlated with the highest and lowest satisfaction products.  These results may help identify areas for Lenovo to focus on further investments that can improve long-term customer satisfaction.

\section{Predictive Model}

Our team considered three approaches to developing a predictive model--linear regression, proportional odds model and time series analysis--because these seemed potentially suitable for this problem and the team had previous experience with these modeling approaches.  We ultimately chose a linear model which will be described in detail here; a summary of the other methods and why we do not recommend them at this time can be found in Appendix \ref{app:PredModel}.

We fit numerous linear models using combinations of the options listed below:
\begin{enumerate}[noitemsep]
  \item Epoch: raw data grouped by week, bi-monthly and monthly
  \item Segment: single segment (commercial and consumer combined) versus separate segments
  \item Predictors: Product Sentiment Index (PSI), average star rating, Product Series, Product Name and interactions between predictors
\end{enumerate}

We then evaluated each candidate model in a two-step process.  First, we examined residuals plots of each model to determine if key assumptions (e.g., normality and homogeneity of variance) of a linear model were satisfied.  Aggregating data weekly or bi-monthly often meant there were very few data points per epoch and/or there would be significant outliers which meant that one or both of the key assumptions were not satisfied.  Therefore we determined aggregating data by month yielded better results for fitting the linear model.  The second phase of our analysis used F-tests/Analysis of Variance (ANOVA) to identify predictors in the full model (PSI, average stars, Series and Series*PSI interaction) which did not significantly improve the model fit and thus could be removed to yield a more parsimonious model.  Detailed steps in the model selection process can also be found in Appendix \ref{app:LinModel}.  Our final recommendation is two different models for the consumer and commercial segments:

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width="75%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(error = FALSE)

library(tidyverse)
library(lubridate)
library(grid)
library(gridExtra)
# library(kableExtra)
library(tidyverse)
# library(kableExtra)
library(car)
library(caret)
library(pander)
source("../src_lenovo_project.R")
source("../LenovoAnalysis/data_prep.R")
source("../LenovoAnalysis/helper_calcs.R")
source('../LenovoAnalysis/MDP_lenovo_helper.R', encoding = 'UTF-8')
filter_raw_data()

load("../CleanData/filtered_sentiment_data.Rdata")
load("../CleanData/filtered_survey_data.Rdata")
load("../CleanData/filtered_product_lists.Rdata")
load("../CleanData/timeSeries.Rdata")
load("../CleanData/correlation.Rdata")
```

```{r}
# Consumer Only
psi <- sentiment.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_consumer <- inner_join(psi, nps) %>%
  filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Change SeriesName to factor
#all_consumer$SeriesName <- factor(all_consumer$SeriesName)

# Fit model without interaction term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl2 <- train(nps ~ SeriesName + psi + stars, data=all_consumer,
              method = "lm", trControl = train.control)
```

```{r}
# Commercial Only
psi <- sentiment.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_comm <- inner_join(psi, nps) %>%
  #filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Commercial model without stars
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl4 <- train(nps ~ SeriesName*psi, data=all_comm,
              method = "lm", trControl = train.control)
```

$$NPS_{consumer} = (2.43 + \beta_1) - 0.16 * PSI + 13.42 * stars \text{    (1)}$$
$$NPS_{commercial} = (38.87 + \beta_1) + (\beta_2 - 0.49) * PSI \text{    (2)}$$

where the $\beta_i$ are listed in Appendix \ref{app:coeff} due to the large number of Series catgeories.

After model selection, we used k-fold cross validation to estimate the prediction performance of the selected models. Although the model selection process did result in statistically significant coefficients for PSI and stars, the results from the k-fold cross validation show that both models still leave significant room for improvement based on the Root Mean Square Error and Mean Absolute Error (Table \ref{tbl:KFold}).

\begin{table}[ht]
\centering
\caption{Summary of K-fold Cross-Validation Results}
\begin{tabular}{l|c|c|c|}
\cline{2-4}
\multicolumn{1}{c|}{}                  & F-test p-value   & RMSE  & MAE    \\ \hline
\multicolumn{1}{|l|}{Consumer Model}   & \textless{}0.001 & 22.74 & 17.96  \\ \hline
\multicolumn{1}{|l|}{Commercial Model} & \textless{}0.001 & 25.43 & 19.29 \\ \hline
\end{tabular}
\label{tbl:KFold}
\end{table}

It is perhaps easier to understand these results by comparing plots of observed versus predicted NPS.  Figures \ref{fig:mdlConsumer} and \ref{fig:mdlComm} are scatterplots for the Consumer and Commercial data, respectively.  In both sets of plots, the grey points are the observed (NPS, PSI) scores, the grey line is the regression line for the marginal model $NPS = \beta_o + \beta_1 * PSI$ and the red dots are the _predicted values_ from equations (1) and (2) respectively.  While the predicted (red) points follow the general trend of the observed (grey) points, there is still significant variation that is not accounted for by the models.  If Lenovo started archiving the telemetry data, then new models incorporating the telemetry could be fit once several months of data is available; it is entirely possible incorporating telemetry data into the model could reduce the unaccounted for variation and give better predictions.

```{r fig.cap="\\label{fig:mdlConsumer}Marginal Model Plots, Consumer PSI vs. NPS"}
# Look at scatterplots fitted vs actual

cbind(all_consumer, 
      pred2=predict(mdl2$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred2), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

```{r fig.cap="\\label{fig:mdlComm}Marginal Model Plots, Commercial PSI vs. NPS"}
# Look at scatterplots fitted vs actual

cbind(all_comm, 
      pred4=predict(mdl4$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred4), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

Finally, Lenovo identified five products our model will be tested against: Yoga C930, Ideapad 330s, Yoga 730, Legion Y730 and T490.  The predicted NPS scores and both 95%  confidence and prediction intervals can be found in Appendix \ref{app:preds}.  A prediction interval is always wider than a confidence interval because it includes both the variation in the regression model and the variation in the error term.  However, the prediction interval is more relevant because Lenovo is interested in the predicted NPS score (as opposed to the predicted _mean_ NPS score in which case the confidence interval would be preferred).

\section{Markov Decision Process}

The next phase of our analysis developed a Markov decision process (MDP) to identify optimal policies based on customer sentiment or satisfaction.  Where Lenovo did not identify specific components (e.g., actions, rewards) necessary for the MDP, we will make assumptions and identify them as such.  For this section we will focus on the the MDP for the Consumer segment.  The same analysis process would apply equally to the Commercial segment.

\subsection{States}\label{sec:statedef}

We considered using either PSI or predicted NPS  states as the basis for the MDP.  The challenge with using PSI is that defining states is somewhat arbitrary.  We considered standardizing the PSI scores and then delineating states using multiples of the standard deviation.  However, we felt that approach would not be as intuitive to understand because the states would not be defined according to the [-100, 100] scale Lenovo is familiar with for PSI.  On the other hand, there are established industry-standard categories for NPS [1].  Therefore, we chose to use these industry-standards as the basis for the predicted NPS states in our MDP, and they are listed as follows:Very Low (--): $NPS \le 8$ ; Moderately Low (-): $8 < NPS \le 35$; Normal (0): $35 < NPS \le 45$; Moderately High (+): $45 < NPS \le 65$; Very High (++): $65 < NPS$.

We will include telemetry status in the MDP since Lenovo would like to make use of telemetry in the future.  We will assume there are two possible telemetry states, Normal (N) and Abnormal (A).  See Appendix \ref{app:TM} for an example of how Normal/Abnormal states can be characterized once Lenovo has archived several months worth of data. Therefore the possible combined states for the MDP are {(++,N), (++,A), (+,N), (+,A), (0,N), (0,A), (-,N), (-,A), (--,N), (--,A)}

\subsection{Actions}

We assume two possible actions: {Do Nothing, Conduct Root Cause Analysis}.  Further, we will assume that a root cause analysis would only be considered if the NPS state is Very Low or Moderately Low.

\begin{table}[h!]
\centering
 
\caption{Example Actions}
\begin{tabular}{lc|c|c|c|c|c|}
\cline{3-7}
\multicolumn{1}{c}{}     &                & \multicolumn{5}{c|}{Applicable NPS States}  \\ \hline
\multicolumn{1}{|c|}{ID} & Action         & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{1}  & Do Nothing                  & Y  & Y & Y      & Y  & Y      \\ \hline
\multicolumn{1}{|l|}{2}  & Conduct Root Cause Analysis & Y  & Y & N      & N  & N       \\ \hline
\end{tabular}
\end{table}

\subsection{Transitions}\label{sec:transitions}

We calculated the natural evolution of NPS by month (i.e., same epoch as linear model) from the survey data.  Since we do not have enough data to calculate the natural evolution of Telemetry states, we will assume a transition matrix as shown below.

\begin{table}[h]
\caption{Individual Transition Matrices, NPS and Telemetry}
 
\centering
\hspace*{\fill}
\begin{tabular}[t]{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & - - & - & 0 & + & ++ \\ \hline
\multicolumn{1}{|l|}{- -} & 0.451    & 0.220    & 0.055   & 0.143 & 0.132    \\ \hline
\multicolumn{1}{|l|}{-} & 0202    & 0.506    & 0.079   & 0.101  & 0.111       \\ \hline
\multicolumn{1}{|l|}{0}   & 0.161   & 0.355  & 0.194     & 0.160   & 0.097       \\ \hline
\multicolumn{1}{|l|}{+} & 0.200  & 0.220    & 0.160     & 0.200  & 0.220       \\ \hline
\multicolumn{1}{|l|}{++} & 0.214    & 0.054    & 0.125     & 0.161    & 0.446      \\ \hline
\end{tabular}
\hfill
\begin{tabular}[t]{l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{}      & N & A  \\ \hline
\multicolumn{1}{|l|}{N}      & 0.75 & 0.25  \\ \hline
\multicolumn{1}{|l|}{A}      & 0.25 & 0.75  \\ \hline
\end{tabular}
\hspace*{\fill}
\label{tbl:P1}
\end{table}

We combined the individual transition matrices to form the transition matrix for our MDP.  This natural evolution for the combined states is equivalent to the "Do Nothing" action.

```{r}

tmp <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  calcNPS()

# Define NPS categories
VERY_HIGH <- 60.0
MOD_HIGH <- 45.0
NORMAL <- 35.0
MOD_LOW <- 8.0

res <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  NPSTransitions(VERY_HIGH, MOD_HIGH, NORMAL, MOD_LOW)

p.s = as.matrix(res$P[,2:6])

# Define telemetry matrix
p.tel = matrix(c(0.75, 0.25, 0.25, 0.75),nrow = 2, ncol = 2, byrow = TRUE)
rownames(p.tel) <- c("N","A")
colnames(p.tel) <- c("N","A")

P1 = combine.matrix(p.s,
                    p.tel)


rownames(P1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")
colnames(P1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")

knitr::kable(round(P1, digits=3), row.names=TRUE, caption = "Consumer, All States, Action = Do Nothing  \\label{tbl:P1}" )

```

<!-- \newpage -->

Next, we assume the action "Conduct Root Cause Analysis" reduces the probability of transitioning to Very_Low by $100p\%$ compared to "Do Nothing" if Telemetry is Abnormal and $75p\%$ if Telemetry is Normal (i.e., root cause analysis is more likely to succeed if there is abnormal telemetry data to troubleshoot).  The change in percentage is evenly distributed among the remaining 4 states.  For example, if $p=0.1$ then the transition matrix for ``Conduct Root Cause Analysis" is shown in Table \ref{tbl:P2}.  Note that only the rows which differ from Table \ref{tbl:P1} are displayed.


```{r}

d <- 0.75
p <- 0.1
n <- 5
P2 <- P1

# Modify VERY_LOW, Normal Telemetry
delta <- d * p * P1[1,1]/sum(P1[1,1:5])
P2[1,1] <- P1[1,1] - delta
P2[1,2] <- P1[1,2] + delta/(n-1)
P2[1,3] <- P1[1,3] + delta/(n-1)
P2[1,4] <- P1[1,4] + delta/(n-1)
P2[1,5] <- P1[1,5] + delta/(n-1)
delta <- d * p * P1[1,6]/sum(P1[1,6:10])
P2[1,6] <- P1[1,6] - delta
P2[1,7] <- P1[1,7] + delta/(n-1)
P2[1,8] <- P1[1,8] + delta/(n-1)
P2[1,9] <- P1[1,9] + delta/(n-1)
P2[1,10] <- P1[1,10] + delta/(n-1)

# Modify MOD_LOW, Normal Telemetry
delta <- d *p * P1[2,1]/sum(P1[2,1:5])
P2[2,1] <- P1[2,1] - delta
P2[2,2] <- P1[2,2] + delta/(n-1)
P2[2,3] <- P1[2,3] + delta/(n-1)
P2[2,4] <- P1[2,4] + delta/(n-1)
P2[2,5] <- P1[2,5] + delta/(n-1)
delta <- d *p * P1[2,6]/sum(P1[2,6:10])
P2[2,6] <- P1[2,6] - delta
P2[2,7] <- P1[2,7] + delta/(n-1)
P2[2,8] <- P1[2,8] + delta/(n-1)
P2[2,9] <- P1[2,9] + delta/(n-1)
P2[2,10] <- P1[2,10] + delta/(n-1)

# Modify VERY_LOW, Abnormal Telemetry
delta <- p * P1[6,1]/sum(P1[6,1:5])
P2[6,1] <- P1[6,1] - delta
P2[6,2] <- P1[6,2] + delta/(n-1)
P2[6,3] <- P1[6,3] + delta/(n-1)
P2[6,4] <- P1[6,4] + delta/(n-1)
P2[6,5] <- P1[6,5] + delta/(n-1)
delta <- p * P1[6,6]/sum(P1[6,6:10])
P2[6,6] <- P1[6,6] - delta
P2[6,7] <- P1[6,7] + delta/(n-1)
P2[6,8] <- P1[6,8] + delta/(n-1)
P2[6,9] <- P1[6,9] + delta/(n-1)
P2[6,10] <- P1[6,10] + delta/(n-1)

# Modify MOD_LOW, Normal Telemetry
delta <- p * P1[7,1]/sum(P1[7,1:5])
P2[7,1] <- P1[7,1] - delta
P2[7,2] <- P1[7,2] + delta/(n-1)
P2[7,3] <- P1[7,3] + delta/(n-1)
P2[7,4] <- P1[7,4] + delta/(n-1)
P2[7,5] <- P1[7,5] + delta/(n-1)
delta <- p * P1[7,1]/sum(P1[7,6:10])
P2[7,6] <- P1[7,6] - delta
P2[7,7] <- P1[7,7] + delta/(n-1)
P2[7,8] <- P1[7,8] + delta/(n-1)
P2[7,9] <- P1[7,9] + delta/(n-1)
P2[7,10] <- P1[7,10] + delta/(n-1)

knitr::kable(round(P2[c(1,2,6,7),], digits=3), row.names=TRUE,
             caption = "Consumer, All States, Action = Root Cause Analysis \\label{tbl:P2}" )  

```

\subsection{Rewards}
\begin{table}[h!]
\centering
\caption{Reward Matrix Template}
\subfloat[NPS Reward Matrix]{\begin{tabular}[t]{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & - - & - & 0 & + & ++ \\ \hline
\multicolumn{1}{|l|}{- -} & -2r1    & -r1    & 0   & r1 & 2r1    \\ \hline
\multicolumn{1}{|l|}{-} & -2r1    & -r1    & 0   & r1  & 2r1       \\ \hline
\multicolumn{1}{|l|}{0}   & -2r1   & -r1  & 0     & r1   & 2r1       \\ \hline
\multicolumn{1}{|l|}{+} & -2r1  & -r1    & 0     & r1  & 2r1       \\ \hline
\multicolumn{1}{|l|}{++} & -2r1    & -r1    & 0     & r1    & 2r1      \\ \hline
\end{tabular}\label{tbl:R1}}
\quad
\hspace*{0.5in}
\subfloat[NPS Reward Matrix]{\begin{tabular}[t]{l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{}      & N & A  \\ \hline
\multicolumn{1}{|l|}{N}      & 2r2 & -r2  \\ \hline
\multicolumn{1}{|l|}{A}      & r2 & -2r2  \\ \hline
\end{tabular}\label{tbl:R2}}

\end{table}
We assume rewards are of the form shown in Tables \ref{tbl:R1} and \ref{tbl:R2} with parameters $r1$ and $r2$. We will assume $r1=10$, $r2=5$ and conducting a root cause analysis has cost $c=7$. Note: The combined reward matrices using these parameters can be found in Appendix \ref{app:Rewards}.

\subsection{Results}

Given the previous assumptions, we analyzed the MDP for $p \in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]$ and a discount factor of $0.95$.  The results as a function of $p$ are listed in Table \ref{tbl:MDP}, where "-" represents "Do Nothing" and "RC" represents "Conduct Root Cause Analysis".  The R source code used to conduct this analysis are listed in Appendices \ref{app:MDP} and \ref{app:Addl}; the values for rewards, cost, and discount factor are parameters which can be easily modified by Lenovo.

\begin{table}[h!]
\caption{Optimal Policy as a Function of $p$}
\centering
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
\multicolumn{1}{c|}{} & - -, N & -, N & 0, N & +, N & ++, N & - -, A & -, A & 0, A & +, A & ++, A \\ \hline
\multicolumn{1}{|l|}{$p \le 0.2$} & - & - & - & - & - & - & - & - & - & - \\ \hline
\multicolumn{1}{|l|}{$p = 0.3$} & - & - & - & - & - & RC & - & - & - & - \\ \hline
\multicolumn{1}{|l|}{$p \ge 0.4$} & RC & - & - & - & - & RC & - & - & - & - \\ \hline
\end{tabular}
\label{tbl:MDP}
\end{table}

\section{Customer Sentiment Analysis}

In order to identify areas that are faring well with the population and those that need improvement, it is useful to tap into large datasets for information on potential areas of improvement.  We first identify product-category pairs that correlate with NPS with a correlation of 0.3. Figures \ref{fig:neg-corr} and \ref{fig:neg-corr2} highlight the categories that correlate with NPS.  For Consumer products, lowest NPS was observed for Ideapad 110, the Yoga 710 and the Ideapad 120S; for Commercial products, the lowest was E585, E485 and E580. The recommendations are listed in Table \ref{tab:recom}.  Details on the number of epochs over which correlation was observed is given in Appendix \ref{app:details}.

```{r,  eval=TRUE}
## Sentiment of consumer products
psi.study <- sentiment.consumer %>%
  mutate(ProductName = I(toupper(as.character(Product)))) %>%  
  group_by(Category) %>%
  summarise(pos = sum(Sentiment == "POSITIVE"),
            neg = sum(Sentiment == "NEGATIVE"), total = n()) %>%
  mutate(psi.all = 100 * (pos - neg)/(pos +  neg)) %>% ungroup() %>% drop_na() %>% filter(total>50)

nps.study <- survey.consumer%>%
  mutate(ProductName = I(toupper(as.character(Product))), 
         SeriesName = I(toupper(as.character(Series)))) %>%
  mutate(NPS = as.numeric(as.character(Product.NPS))) %>% group_by(ProductName) %>%
  summarise(promoter = sum(NPS >= 9),
            detractor = sum(NPS <= 6),
            total = n()) %>%
  mutate(nps = 100 * (promoter - detractor) / total) %>%  ungroup() %>% drop_na() %>% filter(total>50)
rel.prod = nps.study[order(nps.study$nps)[1:3],c(1,5)]
# merge(summary.cat,rel.prod,by = "ProductName") %>% filter (correlation>0.3)

## Sentiment of Commercial Products

psi.study2 <- sentiment.comm %>%
  mutate(ProductName = I(toupper(as.character(Product)))) %>%  
  group_by(Category) %>%
  summarise(pos = sum(Sentiment == "POSITIVE"),
            neg = sum(Sentiment == "NEGATIVE"), total = n()) %>%
  mutate(psi.all = 100 * (pos - neg)/(pos +  neg)) %>% ungroup() %>% drop_na() %>% filter(total>50)

nps.study2 <- survey.comm %>%
  mutate(ProductName = I(toupper(as.character(Product))), 
         SeriesName = I(toupper(as.character(Series)))) %>%
  mutate(NPS = as.numeric(as.character(Product.NPS))) %>% group_by(ProductName) %>%
  summarise(promoter = sum(NPS >= 9),
            detractor = sum(NPS <= 6),
            total = n()) %>%
  mutate(nps = 100 * (promoter - detractor) / total) %>%  ungroup() %>% drop_na() %>% filter(total>50)

rel.prod2 = nps.study2[order(nps.study2$nps)[1:3],c(1,5)]
recom = rbind(merge(summary.cat,rel.prod,by = "ProductName") %>% filter (correlation>0.3),
              merge(summary.cat2,rel.prod2,by = "ProductName") %>% filter (correlation>0.3))
# ggplot(recom)+geom_col(aes(x = ProductName, y= n, fill = Category))+theme_gray(base_size = 9)+theme(legend.position = "bottom")+labs(fill= "")+xlab("Product Name")+ylab("# of Categories")
summ = recom %>% group_by(ProductName) %>% summarise(Cateogry.sum = tolower(x = paste(unlist(Category), collapse =", ")),
                                              Length = paste(unlist(size), collapse =", "),
                                              Correlation = paste(unlist(round(correlation,2)), collapse =", "))
# summ = cbind(data.frame(Segment = c(rep("Commercial",3), rep("Consumer", 3))), summ)
pander::pander(summ[,c(1,2)], row.names = FALSE,caption = "\\label{tab:recom} Summary of recommendations", col.names = c("Product", "Category"))
```

\section{Conclusion and Recommendations}

We developed two linear models, one for the Consumer segment and one for the Commercial segment, to predict NPS from a combination of Series, PSI and average star ratings.  While there is predictive values in the customer sentiment data, there is still a significant variation in the NPS scores that is not accounted for by our models.  Since we only had 30 days of telemetry data, that was not sufficient to incorporate into our models.  However, we hypothesize that if Lenovo archives sufficient telemetry data (6-12 months) and builds new models incorporating that data, then it may give better predictions.  

We also developed a framework for a Markov Decision Process (MDP) based on the predicted NPS score and categories of Normal vs. Abnormal telemetry activity (assuming that Lenovo will begin archiving the telemetry data).  Because we had to make many assumptions (e.g., potential actions Lenovo might take, the rewards/costs associated with those actions) in order to formulate the MDP, the _specific_ results of the MDP are of less importance to Lenovo than the analysis process and the framework itself.  We have provided all of the source code used to build and analyze the MDP in the Appendices, and all of the assumptions are parameters in the source code so that Lenovo could easily modify these tools for future use. 

Finally, we identified the three products with the lowest average NPS score in each business segment.  These are the Ideapad 110, the Yoga 710 and the Ideapad 120S for the Consumer segment, and the E585, E485 and E580 for the Commercial segment.  Further, we identified the categories of customer sentiment with the strongest correlation to PSI for each of the lowest performing products.  Some of the categories common to multiple low-rated products include Battery, Memory and Display.  While we cannot conclude improvements in these categories will improve PSI and NPS scores based on the data available (i.e., correlation does not imply causation), the strong correlation does justify Lenovo considering these categories as areas for potential improvement. 

\clearpage

\section*{References}

[1] North America International. Net promoter benchmarks. [www.satmetrix.com/wpcontent/
uploads/2019/04/2019Benchmarks.pdf](www.satmetrix.com/wpcontent/uploads/2019/04/2019Benchmarks.pdf), 2019. [Online; accessed 19 Nov 2019].

[2] George EP Box, Gwilym M. Jenkins, Gregory C. Reinsel, and Greta M Ljung. _Time series analysis: forecasting and control_. John Wiley & Sons, 2015.
<!-- \bibliographystyle{unsrt} -->
<!-- \bibliography{ise560report} -->

\clearpage

\appendix

\section{Alternative Model Approach}\label{app:PredModel}

\subsection{Time Series Analysis}

Considering the pNPS score as a stochastic process where the random variables are realizations of pNPS value, attempts to perform classical  time-series analysis.  If $X_t$ denoted pNPS at time $t$, the auto-regressive moving-average(ARMA) model [2] would be

$$X_{t} = \sum_{i = 1}^p \alpha_i X_{t-i} + \sum_{j = 1}^q  \beta_j \theta_{t-j} $$.

where $\alpha_i$ $\forall i \in \{1,2,\ldots, p\}$  and $\beta_j$ $\forall j \in \{1,2,\ldots, q\}$ are coefficients of the linear model making a 1-step prediction of pNPS,  $\theta_t$ is the moving average component at time $t$ following a normal distribution with zero mean and variance $\sigma^2_w$. As the pNPS scores are heteroscedastic, the time series are also differenced $d$ times to satisfy the condition of stationarity.  Note that $p$, $d$ and $q$ are  referred to as the order of the model.  In Tables \ref{tab:ARIMA_Commercial} and \ref{tab:ARIMA_Consumer}, we present train set errors of best ARIMA model with the lowest Akaike information criterion (AIC) with order $(p,q,r)$.  The pNPS calculations were made over a period of seven days.  It is also observed that temporal correlation is sensitive to the series.

```{r}
series.ts.main.com <- series.ts.main.com%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0,SeriesName != "ACCESSORIES", SeriesName != "ANNIVERSARY")
series.ts.main <- series.ts.main%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0, SeriesName != "SMART HOME", SeriesName != "OTHER")
knitr::kable(series.ts.main.com[series.ts.main.com$Duration == 7,-5],row.names = FALSE,caption = "\\label{tab:ARIMA_Commercial} Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))
knitr::kable(series.ts.main[series.ts.main$Duration == 7,-5],row.names = FALSE,caption = "\\label{tab:ARIMA_Consumer} Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE") )


```


While the intention is analyze the statistical relationship between adjacent samples of pNPS calculations, the epoch over which the pNPS scores are varied from 1 to 30 days.  Figures \ref{fig:arima.comm} and \ref{fig:arima.con} shows the Mean Absolute Errors as a function of the epoch over which the pNPS score was calculated.  It is observed for a majority of the Series, the surveys are least correlated with a two-week window.

```{r figs, echo = FALSE,  fig.cap = "\\label{fig:arima.comm} Train set pNPS prediction errors vs window size for commercial data.", fig.width = 8}
plt.com <- ggplot(series.ts.main.com, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.com
```

```{r figs_con, echo = FALSE, fig.cap="\\label{fig:arima.con} Train set pNPS prediction errors vs window size for consumer data.", fig.width = 8}
plt.con <- ggplot(series.ts.main, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.con
```

\clearpage

\section{Linear Model Selection}\label{app:LinModel}

\subsection{Consumer Model}

```{r echo=TRUE}
# Consumer Only
psi <- sentiment.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, 
                                     format = "%m/%d/%y"), 
                             unit = "month")) %>%
  calcPSI()

nps <- survey.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, 
                                     format = "%m/%d/%Y"), 
                             unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all <- inner_join(psi, nps) %>%
  filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Fit full model with interation term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl1 <- train(nps ~ SeriesName*psi + stars, data=all,
              method = "lm", trControl = train.control)

# Fit model without interaction term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl2 <- train(nps ~ SeriesName + psi + stars, data=all,
              method = "lm", trControl = train.control)

par(mfrow=c(2,2))
plot(mdl2$finalModel)

res <- anova(mdl2$finalModel, mdl1$finalModel)
```

The F-statistic comparing the full and restricted models is 1.402 with 1 and 119 degrees of freedom.  The p-value is 0.168.  Therefore we fail to reject the null hypothesis, and we conclude the additional predictors in the full model do not significantly improve the fit.

\subsection{Commercial Model}

```{r echo=TRUE}
# Commercial Only
psi <- sentiment.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, 
                                     format = "%m/%d/%y"), 
                             unit = "month")) %>%
  calcPSI()

nps <- survey.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, 
                                     format = "%m/%d/%Y"), 
                             unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all <- inner_join(psi, nps) %>%
  #filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Fit full model with interaction term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl3 <- train(nps ~ SeriesName*psi + stars, data=all,
              method = "lm", trControl = train.control)

# Fit model without stars
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl4 <- train(nps ~ SeriesName*psi, data=all,
              method = "lm", trControl = train.control)

par(mfrow=c(2,2))
plot(mdl4$finalModel)

res <- anova(mdl3$finalModel, mdl4$finalModel)
```

The F-statistic comparing the full and restricted models is 1.3025 with 1 and 106 degrees of freedom.  The p-value 0.2563.  Therefore we fail to reject the null hypothesis, and we conclude the additional predictors in the full model do not significantly improve the fit.


\clearpage

\section{Coefficients for Recommended Linear Models}\label{app:coeff}


```{r}
# Consumer
beta_coeff <- round(mdl2$finalModel$coefficients[2:14], digits=2)
nconsumer <- sub("SeriesName", "", names(beta_coeff))
knitr::kable(data.frame(nconsumer, unname(beta_coeff)),
             col.names = c("Consumer Series", "$\\beta_1$"),
             caption = "Coefficients for Consumer Model" ) 

# Commercial
beta_coeff1 <- round(mdl4$finalModel$coefficients[2:12], digits=2)
beta_coeff2 <- round(mdl4$finalModel$coefficients[14:24], digits=2)
ncomm <- sub("SeriesName", "", names(beta_coeff1))
knitr::kable(data.frame(ncomm, unname(beta_coeff1), unname(beta_coeff2)), 
             col.names = c("Commercial Series", "$\\beta_1$", "$\\beta_2$"),
             caption = "Coefficients for Commercial Model" ) 

```

\clearpage

\section{Predictions for Key Products}\label{app:preds}

The following tables list predicted NPS (pNPS) scores, 95% confidence interval bounds and 95% prediction interval bounds for five key products previously identified by Lenovo.

```{r}
# Can't get prediction interval from caret so refit directly with lm
pmdl2 <- lm(nps ~ SeriesName + psi + stars, data=all_consumer)

#### YOGA C930 ###
yogac930 <- sentiment.consumer.all %>%
  filter(ProductName == "YOGA C930") %>%
  mutate(SeriesName = "YOGA") %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI() %>%
  ungroup()

yogac930.pred <- predict(pmdl2, newdata=yogac930, interval="prediction")
yogac930.conf <- predict(pmdl2, newdata=yogac930, interval="confidence")

yogac930.summ <- cbind(Date = yogac930$date,
                       PSI = round(yogac930$psi, digits=2),
                       data.frame(pNPS = round(yogac930.conf[,1], digits=2),
                                            CI.lower = round(yogac930.conf[,2], digits=2),
                                            CI.upper = round(yogac930.conf[,3], digits=2),
                                            PI.lower = round(yogac930.pred[,2], digits=2),
                                            PI.upper = round(yogac930.pred[,3], digits=2))) 

knitr::kable(yogac930.summ, caption="Yoga C930 Predictions, 95\\% Confidence and Prediction Intervals",booktabs = T)
```

\subsection{Ideapad 330S}
```{r}
### Ideapad 330 ###
ideapad330 <- sentiment.consumer.all %>%
  filter(ProductName == "IDEAPAD 330S 14" |
           ProductName == "IDEAPAD 330S 15" |
           ProductName == "IDEAPAD 330S GENERAL") %>%
  mutate(SeriesName = "IDEAPAD 300 SERIES") %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI() %>%
  ungroup()

ideapad330.pred <- predict(pmdl2, newdata=ideapad330, interval="prediction")
ideapad330.conf <- predict(pmdl2, newdata=ideapad330, interval="confidence")

ideapad330.summ <- cbind(Date = ideapad330$date,
                       PSI = round(ideapad330$psi, digits=2),
                       data.frame(pNPS = round(ideapad330.conf[,1], digits=2),
                                            CI.lower = round(ideapad330.conf[,2], digits=2),
                                            CI.upper = round(ideapad330.conf[,3], digits=2),
                                            PI.lower = round(ideapad330.pred[,2], digits=2),
                                            PI.upper = round(ideapad330.pred[,3], digits=2))) 

knitr::kable(ideapad330.summ, caption="Ideapad 330 Predictions, 95\\% Confidence and Prediction Intervals" )
```

\subsection{Yoga 730}
```{r}
### Yoga 730 ###

yoga730 <- sentiment.consumer.all %>%
  filter(ProductName == "YOGA 730 13" |
           ProductName == "YOGA 730 15" |
           ProductName == "YOGA 730 GENERAL") %>%
  mutate(SeriesName = "YOGA") %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI() %>%
  ungroup()

yoga730.pred <- predict(pmdl2, newdata=yoga730, interval="prediction")
yoga730.conf <- predict(pmdl2, newdata=yoga730, interval="confidence")

yoga730.summ <- cbind(Date = yoga730$date,
                       PSI = round(yoga730$psi, digits=2),
                       data.frame(pNPS = round(yoga730.conf[,1], digits=2),
                                            CI.lower = round(yoga730.conf[,2], digits=2),
                                            CI.upper = round(yoga730.conf[,3], digits=2),
                                            PI.lower = round(yoga730.pred[,2], digits=2),
                                            PI.upper = round(yoga730.pred[,3], digits=2))) 

knitr::kable(yoga730.summ, caption="Yoga 730 Predictions, 95\\% Confidence and Prediction Intervals" )
```

\subsection{Legion Y730}
```{r}
### Legion Y730 ###

legiony730 <- sentiment.consumer.all %>%
  filter(ProductName == "LEGION Y730 15" |
           ProductName == "LEGION Y730 17" |
           ProductName == "LEGION Y730 GENERAL") %>%
  mutate(SeriesName = "LEGION (Y SERIES)") %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI() %>%
  ungroup()

legiony730.pred <- predict(pmdl2, newdata=legiony730, interval="prediction")
legiony730.conf <- predict(pmdl2, newdata=legiony730, interval="confidence")

legiony730.summ <- cbind(Date = legiony730$date,
                       PSI = round(legiony730$psi, digits=2),
                       data.frame(pNPS = round(legiony730.conf[,1], digits=2),
                                            CI.lower = round(legiony730.conf[,2], digits=2),
                                            CI.upper = round(legiony730.conf[,3], digits=2),
                                            PI.lower = round(legiony730.pred[,2], digits=2),
                                            PI.upper = round(legiony730.pred[,3], digits=2))) 

knitr::kable(legiony730.summ, caption="Legion Y730 Predictions, 95\\% Confidence and Prediction Intervals" )

```

\subsection{T490}

```{r}
pmdl4 <- lm(nps ~ SeriesName*psi, data=all_comm)
               
### Legion Y730 ###

t490 <- sentiment.comm.all %>%
  filter(ProductName == "T490") %>%
  mutate(SeriesName = "T SERIES") %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI() %>%
  ungroup()

t490.pred <- predict(pmdl4, newdata=t490, interval="prediction")
t490.conf <- predict(pmdl4, newdata=t490, interval="confidence")

t490.summ <- cbind(Date = t490$date,
                       PSI = round(t490$psi, digits=2),
                       data.frame(pNPS = round(t490.conf[,1], digits=2),
                                            CI.lower = round(t490.conf[,2], digits=2),
                                            CI.upper = round(t490.conf[,3], digits=2),
                                            PI.lower = round(t490.pred[,2], digits=2),
                                            PI.upper = round(t490.pred[,3], digits=2)))

knitr::kable(t490.summ, caption="T490 Predictions, 95\\% Confidence and Prediction Intervals" )

```

\clearpage

\section{MDP Rewards Matrices}\label{app:Rewards}

Assuming $r1=10$, $r2=5$ and $c=7$, the combined reward matrices for each action are as follows:

```{r}

###### Define Rewards Matrices ######
# Select parameters
n <- 5
r1 <- 10
r2 <- 5
cost <- 7
# Do Nothing
Rnps <- matrix(rep(c(-2*r1, -1*r1, 0, r1, 2*r1),n),
               nrow=n,
               byrow=TRUE)
Rtel <- matrix(c(2*r2, r2, -r2, -2*r2), nrow=2)

R_A1 <- combine.matrix(Rnps, Rtel, TRUE)
rownames(R_A1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N",
                    "- - , A"," -, A"," 0, A"," +, A","++, A")
colnames(R_A1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N",
                    "- - , A"," -, A"," 0, A"," +, A","++, A")

knitr::kable(R_A1, row.names=TRUE, caption = "Consumer, Rewards, Action = Do Nothing" )
```

```{r}
# Root Cause Analysis
# Select parameters
r1 <- 10
r2 <- 5
cost <- 7
# Set cost to "infinity" for states where action is not available
INF <- 9999
R_A2 <- matrix(-INF, nrow=nrow(R_A1), ncol=ncol(R_A1))
R_A2[1,] <- R_A1[1,] - cost
R_A2[2,] <- R_A1[2,] - cost
R_A2[6,] <- R_A1[6,] - cost
R_A2[7,] <- R_A1[7,] - cost

rownames(R_A2) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")
colnames(R_A2) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")

knitr::kable(R_A2[c(1,2,6,7),], row.names=TRUE, caption = "Consumer, Rewards, Action = Root Cause Analysis" )

```

\clearpage

\section{MDP Analysis Source Code}\label{app:MDP}

\lstinputlisting{../LenovoAnalysis/MDP_final.R}

\clearpage

\section{Additional R code}\label{app:Addl}

\subsection{data\_prep.R}

\lstinputlisting{../LenovoAnalysis/data_prep.R}

\subsection{helper\_calcs.R}

\lstinputlisting{../LenovoAnalysis/helper_calcs.R}

\clearpage

\section{Estimating Telemetry States}\label{app:TM}

Assuming a time-series of driver health is available for a particular driver, states can be defined between normal $N$ and abnormal $AN$ operations depending on a performance score $ c_t $ at time $t$ and is given in Equation.\ref{crash_rate_score}.
\begin{equation}\label{crash_rate_score}
c_{t} = min\left\{1,\frac{\text{\# Crashes}}{\text{Total machines }\times \text{ Percent Impacted}}\right\}
\end{equation}
Note that our results would be more relevant if we have access to the number of unique machines that have experienced a crash.  This is can defined as 
$$c_t = \frac{\text{ Number of unique machines crashing at time $t$ }}{\text{Total Number of Machines Impacted at time $t$}}$$

When the performance score is above a threshold, we flag the observations as Anomalous. The transition matrix for telemetry can be constructed similar to the pNPS transition matrix by observing the number of transitions from each state for a given driver update.   specific driver updates (Two states $S_T \in \{N, AN\}$) and is referred to as $P_{T}$.  For the sake of demonstration, we assume the following transition matrix for telemetry:

```{r, echo=FALSE, eval=TRUE}

n.tel = 2
p.tel = matrix(c(0.75, 0.25, 0.25, 0.75),nrow = 2, ncol = 2, byrow = TRUE)
rownames(p.tel) <- c("N","AN")
colnames(p.tel) <- c("N","AN")
knitr::kable(p.tel ,  caption = "A possible telemetry transition matrix from ",  row.names = TRUE )

```

The final state matrix $P$ would be a combination of $P_S$ and $P_T$ as shown in Section \ref{sec:transitions}.

\clearpage

\section{Customer Sentiment Analysis}

\subsection{Category-wise Correlation Between NPS and PSI}

```{r eval=TRUE, fig.cap = "\\label{fig:neg-corr} Correlation between NPS and PSI for Customer Series", fig.width = 8, fig.pos="H"}
# slice.low %>% ggplot(aes(x = psi, y = nps, group = SeriesName))+geom_point()+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)

slice.high %>% ggplot(aes(x = psi, y = nps, group = ProductName))+geom_point()+theme(legend.position = "none")+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)+xlab("PSI")+ylab("pNPS")


```

```{r eval=TRUE, fig.cap = "\\label{fig:neg-corr2} Correlation between NPS and PSI for Commercial Series", fig.width = 8, fig.pos="H"}
# slice.low %>% ggplot(aes(x = psi, y = nps, group = SeriesName))+geom_point()+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)

slice.high2 %>% ggplot(aes(x = psi, y = nps, group = ProductName))+geom_point()+theme(legend.position = "none")+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)+xlab("PSI")+ylab("pNPS")


```

\clearpage

\subsection{Detailed Summary of Recommendations}\label{app:details}
```{r}
pander::pander(summ, row.names = FALSE,caption = "\\label{tab:recom_d} Summary of recommendations", col.names = c("Product", "Category", "Number of Epochs", "Correlation"))

```


