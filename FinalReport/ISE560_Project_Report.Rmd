---
title: "ISE 560 Project Report"
author: "Shawn Markham, Melissa Wong, Shrikanth Yadav & Lekhana Yennam"
date: \today
header-includes:
   - \usepackage{fontspec}
   - \setmainfont{Times New Roman}
   - \usepackage{setspace}
   - \onehalfspacing
   - \usepackage{enumitem}
   - \usepackage{float}
   - \usepackage{url}
   # - \usepackage{biblatex}
   # - \addbibresource{ise560report.bib}
   - \pagenumbering{gobble}
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    fig_caption: true
documentclass: article
fontsize: 12pt
geometry: margin=1in 
urlcolor: blue
---

\begin{figure}[b!]
  \centering
  \includegraphics[width = 0.48\textwidth]{images/ise_logo.pdf}
\end{figure}

\newpage

\pagenumbering{roman}
\setcounter{page}{2}

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage

\pagenumbering{arabic}

\section{Executive Summary}

\section{Introduction}

Lenovo is an international company and a market leader in the personal computer industry.  One tool Lenovo relies upon in assessing customer satisfaction is Net Promoter Score (NPS); this is a standard metric used by major corporations across many industries.  However, latency is a limitation because it can take several months to aggregate sufficient data to calculate NPS.  Therefore our team was tasked with developing a model to predict NPS from more timely information such as customer comments and ratings from websites and telemetry data.

In addition to developing a predictive model for NPS, our team also characterized the natural evolution of both NPS and customer sentiment over time.  These stochastic models form the basis for a Markov Decision Process (MDP) which can be used by Lenovo to identify the optimal actions to take based on the available information.  

Finally, our team analyzed the customer sentiment and comments to identify factors correlated with the highest and lowest satistfaction products.  These results may help identify areas for Lenovo to focus further investments that can improve long-term customer satistifaction. Subsequent sections of this document provide detailed explanations of each phase of our analysis and associated recommendations.

\section{Predictive Model}

Our team considered three approaches to developing a predictive model--linear regression, proportional odds model and time series analysis--because these seemed potentially suitable for this problem and the team had previous experience with these modeling approaches.  We ultimately chose a linear model which will be described in detail here; a summary of the other methods and why we do not recommend them at this time can be found in Appendix \ref{app:PredModel}.

We fit numerous linear models using combinations of the options listed below:
\begin{enumerate}
  \item Epoch: raw data grouped by week, bi-monthly and monthly
  \item Segment: single segment (commercial and consumer combined) versus separate segements
  \item Predictors: Product Sentiment Index (PSI), average star rating, Product Series, Product Name and interactions between predictors
\end{enumerate}

We then evaluated each model in a two-step phase process.  First, we examined residuals plots of each model to determine if key assumptions (normality and homogeneity of variance) of a linear model were satisfied.  Aggregating data weekly or bi-monthly often meant there were very few data points per epoch and/or there would be significant outliers which meant that one or both of the key assumptions were not satisfied.  Thus it was during this phase that we determined aggreating data by month yielded beter results for fitting the linear model.  The second phase of our analysis used F-tests/Analysis of Variance (ANOVA) to identify predictors in the full model (PSI, average stars, Series and Series*PSI interaction) which did not significantly improve the model fit and thus could be removed to yield a more parsimonious model.  Detailed steps in the model selection process can also be found in Appendix \ref{app:PredModel}.  Our final recommendation is two different models for the consumer and commercial segments:

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(out.width="50%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(error = FALSE)

library(tidyverse)
library(lubridate)
library(grid)
library(gridExtra)
#library(kableExtra)
library(tidyverse)
library(car)
library(caret)

source("../src_lenovo_project.R")
source("../LenovoAnalysis/data_prep.R")
source("../LenovoAnalysis/helper_calcs.R")

filter_raw_data()

load("../CleanData/filtered_sentiment_data.Rdata")
load("../CleanData/filtered_survey_data.Rdata")
load("../CleanData/filtered_product_lists.Rdata")
load("../CleanData/timeSeries.Rdata")
```

```{r}
# Consumer Only
psi <- sentiment.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_consumer <- inner_join(psi, nps) %>%
  filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Fit model without interaction term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl2 <- train(nps ~ SeriesName + psi + stars, data=all_consumer,
              method = "lm", trControl = train.control)
```

$$NPS_{consumer} = (2.43 + \beta_1) - 0.16 * PSI + 13.42 * stars$$
$$NPS_{commercial} = (38.87 + \beta_1) + (\beta_2 - 0.49) * PSI$$

where $\beta_i$ is unique for each Series in each segment as shown in the tables below:

```{r}
# Commercial Only
psi <- sentiment.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_comm <- inner_join(psi, nps) %>%
  #filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Commercial model without stars
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl4 <- train(nps ~ SeriesName*psi, data=all_comm,
              method = "lm", trControl = train.control)
```

```{r}
# Consumer
beta_coeff <- round(mdl2$finalModel$coefficients[2:14], digits=2)
nconsumer <- sub("SeriesName", "", names(beta_coeff))
knitr::kable(data.frame(nconsumer, unname(beta_coeff)),
             col.names = c("Consumer Series", "$\\beta_1$"),
             caption = "Coefficients for Consumer Model") 

# Commercial
beta_coeff1 <- round(mdl4$finalModel$coefficients[2:12], digits=2)
beta_coeff2 <- round(mdl4$finalModel$coefficients[14:24], digits=2)
ncomm <- sub("SeriesName", "", names(beta_coeff1))
knitr::kable(data.frame(ncomm, unname(beta_coeff1), unname(beta_coeff2)), 
             col.names = c("Commercial Series", "$\\beta_1$", "$\\beta_2$"),
             caption = "Coefficients for Commercial Model") 

```

TO DO: Figure out how to place tables side by side or if space is tight move to Appendix A

After model selection, we used k-fold cross validation to estimate the accuracy of the model for future predictions. While the model selection process did result in statistically significant coefficients for PSI and stars, the results from the k-fold cross validation show that both model still leave significant room for improvement based on the Root Mean Square Error and Mean Absolute Error (Table \ref{tbl:KFold}).

\begin{table}[ht]
\centering
\caption{Summary of K-fold Cross-Validation Results}
\begin{tabular}{l|c|c|c|}
\cline{2-4}
\multicolumn{1}{c|}{}                  & F-test p-value   & RMSE  & MAE    \\ \hline
\multicolumn{1}{|l|}{Consumer Model}   & \textless{}0.001 & 25.96 & 19.69  \\ \hline
\multicolumn{1}{|l|}{Commercial Model} & \textless{}0.001 & 28.85 & -21.73 \\ \hline
\end{tabular}
\label{tbl:KFold}
\end{table}

It is perhaps easiest to understand these results by comparing plots of observed versus predicted.  Figures \ref{fig:mdlConsumer} and \ref{fig:mdlComm} are scatterplots for the Consumer and Commercial data, respectively.  In both sets of plots, the grey points are the observed (NPS, PSI) scores, the grey line is the regression line for the marginal model $NPS = \beta_o + \beta_1 * PSI$ and the red dots are the _predicted values_ from equations (1) and (2) respectively.  One can clearly see that while the predicted (red) points follow the general trend of the observed (grey) points, there is still significant variation that is not accounted for by the models.  If Lenovo started archiving the telemetry data, then models could be refit once several months of data is available; it is entirely possible incorporating telemetry data into the model could reduce the unaccounted for variation and give even better predictions.

TO DO: Number equations for linear models

```{r fig.cap="\\label{fig:mdlConsumer}Marginal Model Plots, Consumer NPS vs. PSI", fig.pos="H"}
# Look at scatterplots fitted vs actual

cbind(all_consumer, 
      pred2=predict(mdl2$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred2), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

```{r fig.cap="\\label{fig:mdlComm}Marginal Model Plots, Commercial NPS vs. PSI", fig.pos="H"}
# Look at scatterplots fitted vs actual

cbind(all_comm, 
      pred4=predict(mdl4$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred4), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

\section{Markov Decision Process}

The next phase of our analysis was to develop a Markov decision process (MDP) to identify optimal policies based on customer sentiment or satisfaction.  Where Lenovo did not identify specific componenents (e.g., actions, rewards) necessary for the MDP, we will make assumptions and identify them as such.  As much as possible, we parameterized the assumptions so that Lenovo can easily modify our software to their specifications in the future.

\subsection{Natural Evolution of NPS and PSI}

The first step in the MDP is to identify the possible states.  We considered both NPS and PSI.  For NPS, the following are standard categories for the computer/laptop industry \cite{Nps19}: 

\begin{itemize}
  \item Very Low: $NPS \le 8$
  \item Moderately $Low: 8 < NPS \le 35$
  \item Normal: $35 < NPS \le 45$
  \item Moderately High: $45 < NPS \le 65$
  \item Very High: $65 < NPS$
\end{itemize}

\subsection{``As Is" MDP}

\subsection{Future MDP}

We present a framework to use temeletry and sentiment to make decisions on the releasing software interventions using Markov Decision Processes.  In what follows, we describe the states, actions and compute transition probabilities with intervention.

## States

We consider the combination of telemetry and user survey as our states. We present a detailed description of the states and ranges.
<!-- $\{(++,N), (+,N),(0,N),(-,N),(- -,N),(++,AN), (+,AN),(0,AN),(-,AN),(- -,AN)\}$.   -->

### Survey Observations 

NPS values are charasterised based on the industry considerations given in Table \ref{tab:NPS_table} <!--TO DO--> five 20\% quantiles denoted as $S_S \in \{++,+, 0, -, - -\}$) and is referred to as $P_{S}$ where:

  - ++: PSI above 65
  - +: PSI between 60 and 65
  - 0: PSI between 45 and 60
  - -: PSI between 42 and 45
  - - -: PSI below 42

```{r Preparation of sentiment matrix, echo=FALSE, eval=FALSE}

#TO DO:  Something not right with line 252, setting chunk eval=FALSE for now so I can 
# knit the rest of the file
n.sent = 5 
tmp <- survey.consumer.all %>%
  group_by(#ProductName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "week")) %>%
  calcNPS()
# Define PSI categories
VERY_HIGH <- 60.0
MOD_HIGH <- 45.0
NORMAL <- 35.0
MOD_LOW <- 8.0

res <- survey.consumer.all %>%
  group_by(#ProductName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "week")) %>%
  NPSTransitions(VERY_HIGH, MOD_HIGH, NORMAL, MOD_LOW)


tmp = as.matrix(res$P,ncol = 5,nrow = 5)
p.s = matrix(as.numeric(tmp[1:5,2:6]),ncol = 5, nrow= 5) %>% round(digits = 3)
rownames(p.s) = colnames(tmp)[2:6]
colnames(p.s) = colnames(tmp)[2:6]
knitr::kable(res$P, row.names=TRUE, caption = "Transition matrix for consumer surveys (weekly)")

```


### Telemetry

Assuming we have a time-series of telemetry data, states can be defined between normal $N$ and abnormal $AN$ operation.  The no-action transition matrix can be constructed from defining thresholds in the hours of battery life and in the crash-rate related to specific driver updates (Two states $S_T\in \{N, AN\}$) and is referred to as $P_{T}$.

An example of how the telemetry is used is as follows.

```{r, echo=FALSE, eval=FALSE}
# setting eval=FALSE for now until I get csv files so I can knit the rest of the file

telemetry.con <- read.csv("../RawData/Driver_Health_consumer_ideapad.csv", header = TRUE)

# TO DO:  I think there's a cut & paste typo because filename below is identical to one above
telemetry.comm <- read.csv("../RawData/Driver_Health_consumer_ideapad.csv", header = TRUE)

telemetry.con <- telemetry.con %>% mutate(Driver.Name = as.character(Driver.Name),
                                          Crashes = as.numeric(Crashes), 
                                          Percent.Impacted = as.numeric(sub("%", "",Percent.Impacted)),
                                          Total.Machines = as.numeric(Total.Machines)) %>%
  mutate(Percent.crash = Crashes/Total.Machines*100)

q.1 <- quantile(telemetry.con$Percent.crash[telemetry.con$Driver.Name == 'atikmpag.sys'], probs = c(0.25, 0.5, 0.75))
# quantile(telemetry.con$Percent.Impacted[telemetry.con$Driver.Name == 'atikmpag.sys'], probs = c(0.25, 0.5, 0.75))
knitr::kable(q.1, caption = "For the crash-rate percentage, 0.25, 0.5 and 0.75 Quantiles of the atikmpag.sys driver are as follows.")
```

Assuming driver updates are released because of a high crash-rate, we calculate the percent crash-rate from 

$$c_{t} =  \frac{\text{Number Crashes for a specifc driver}}{\text{Number Machines associted with the driver}}$$.

We then define a threshold 50\% quantile level $\gamma$ above which the epoch is labelled as Abnormal operation.

```{r, echo=FALSE, eval=FALSE}

#TO DO: setting eval=FALSE for now so I can knit the rest of the file.
n.tel = 2
p.tel = matrix(c(0.75, 0.25, 0.25, 0.75),nrow = 2, ncol = 2, byrow = TRUE)
rownames(p.tel) <- c("N","AN")
colnames(p.tel) <- c("N","AN")
knitr::kable(p.tel ,  caption = "A possible telemetry transtion matrix from ",  row.names = TRUE)

```


The final state matrix $P$ would be a combination of $P_S$ and $P_T$ given by


$$P_{1}(s,t->s_1,t_1) = P_S(s)P_S(s_1)P_T(t)P_T(t_1)$$

```{r eval=FALSE}
# Calcualtion of combined state probability matrix
P1 = combine.telemetry.sentiment(number.survey.states = n.sent,
                            number.telemetry.states = n.tel,
                            sentiment.probability.matix = p.s, 
                            telemetry.probability.matix = p.tel)
P1 <- round(P1,digits = 3)
rownames(P1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
colnames(P1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
knitr::kable(P1 ,  caption = "A combined Telemetry-Sentiment State transition matrix.",  
             row.names = TRUE)


```


## Actions

The actions are chosen between performing an intervention and doing nothing.  

- A shift towards normal operating mode is expected in the telemetry state transition matrix associated with an intervention.  The transition matrix is denoted as $P_{T,1}$.  As the telemetry is not provided, we assume that the transition probabilities corresponding to positive and negative sentiment will increase and decrease respectively by a predefined factor $\gamma_T$.  Note that $\gamma_T$ can be estimated by observing the percentage of impacted machines due to an intervention.

- A relatively small shift is expected in the sentiment transition matrix as the population is relatively late to react to improvements.  This is referred to as $P_{S,1}$.The new sentiment transition matrix can be estimated similarly by choosing the fraction by which probabilities will change.  This is denoted by $\gamma_S$. Note that $\gamma_S<\gamma_T$.  


The joint transition matrix $P_1$ is associated with the combination of $P_{S,1}$ and $P_{T,1}$.



## Rewards

A cost $r_k$ is incurred when an intervention $k\in\{1,2\}$ is performed.  The no-action cost matrix that incentivize transitions towards higher sentiment scores are denoted by $R_{T,S,1}$.

```{r, echo= FALSE, eval=FALSE}
# TO DO: setting eval=FALSE so I can knit the rest of the file
R.st1 = matrix(c(140, 120, 75, 50, 30, -10, -25, -50, -100, -120,
                 130, 110, 70, 40, 20, -15, -30, -55, -100, -120,
                 120, 100, 65, 30, 10, -25, -35, -60, -100, -120,
                 120, 100, 60, 20, 5, -35, -40, -65, -100, -120,
                 120, 100, 55, 10, 0, -40 ,-45, -70, -100, -120,
                 120, 100, 70, 45, 30, -45 ,-50, -75, -100, -120,
                 120, 100, 65, 40, 20, -50, -55, -80, -100, -120,
                 120, 100, 60, 35, 10, -55, -60, -85, -100, -120,
                 120, 100, 55, 30, 5, -60, -65, -90, -110, -130,
                 120, 100, 50, 25, 0, -65, -70, -95, -120, -140),nrow = n.sent*n.tel, ncol = n.sent*n.tel,byrow = TRUE)
rownames(R.st1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
colnames(R.st1) <- c("++,N","+,N","0,N","-,N","- -,N","++,AN","+,AN","0,AN","-,AN","- -,AN")
knitr::kable(R.st1 ,  caption = "A suggested no-action reward function",  row.names = TRUE)
```

The reward matrix $R_{S,T,2}$ associated with an intervention should be calculated based on $P_{1}$ as $$R_{S,T,2}(s,t->s_1,t_1) = -r_2+\sum_{x}\left(R_{S,T,1}(s,t->x)\right) P_1(s,t->x)$$ where $x\in\{(+,N),(0,N),(-,N),(+,AN),(0,AN),(-,AN)\}$

\section{Customer Sentiment Analysis}

\section{Recommendations}

\clearpage

\bibliographystyle{unsrt}
\bibliography{ise560report}
<!-- \printbibliography -->

\clearpage

\appendix

\section{Predictive Model Approaches}\label{app:PredModel}

\subsection{Linear Model}

\subsection{Proportional Odds Model}

\subsection{Time Series Analysis}

Considering the pNPS score as a stochastic process where the random variables are realizations of pNPS value, attempts to perform classical  time-series analysis.  If $X_t$ denoted pNPS at time $t$, the auto-regressive moving-average(ARMA) model \cite{box2015time} would be

$$X_{t} = \sum_{i = 1}^p \alpha_i X_{t-i} + \sum_{j = 1}^q  \beta_j \theta_{t-j} $$.

where $\alpha_i$ $\forall i \in \{1,2,\ldots, p\}$  and $\beta_j$ $\forall j \in \{1,2,\ldots, q\}$ are coefficients of the linear model making a 1-step prediction of pNPS,  $\theta_t$ is the moving average component at time $t$ following a normal distribution with zero mean and variance $\sigma^2_w$. As the pNPS scores are heteroscedastic, the time series are also differenced $d$ times to satisfy the condition of stationarity.  Note that $p$, $d$ and $q$ are  referred to as the order of the model.  In Tables \ref{tab:ARIMA_Commercial} and \ref{tab:ARIMA_Consumer}, we present train set errors of best ARIMA model with the lowest Akaike information criterion (AIC) with order $(p,q,r)$.  The pNPS calcualtions were made over a period of seven days.  It is also observed that temporal correlation is sensitive to the series.

```{r}
series.ts.main.com <- series.ts.main.com%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>% 
  filter(MAE != 0,SeriesName != "ACCESSORIES", SeriesName != "ANNIVERSARY")
series.ts.main <- series.ts.main%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>% 
  filter(MAE != 0, SeriesName != "SMART HOME", SeriesName != "OTHER") 
knitr::kable(series.ts.main.com[series.ts.main.com$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Commercial", caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))
knitr::kable(series.ts.main[series.ts.main$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Consumer",caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))


```


While the intention is analyze the statistical relationship between adjacent samples of pNPS calculations, the epoch overwhich the pNPS scores are varied from 1 to 30 days.  Figures \ref{fig:arima.comm} and \ref{fig:arima.con} shows the Mean Absolute Errors as a funtion of the epoch over which the pNPS score was calculated.  It is observed for a majority of the Series, the surveys are least correlated with a two-week window.   

```{r figs, echo = FALSE,  fig.cap = "\\label{fig:arima.comm} Train set pNPS prediction errors vs window size for commercial data.", fig.width = 8}
plt.com <- ggplot(series.ts.main.com, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.com
```

```{r figs_con, echo = FALSE, fig.cap="\\label{fig:arima.con} Train set pNPS prediction errors vs window size for consumer data.", fig.width = 8}
plt.con <- ggplot(series.ts.main, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.con
```

