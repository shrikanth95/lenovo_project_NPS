---
title: "ISE 560 Project Report"
author: "Shawn Markham, Melissa Wong, Shrikanth Yadav & Lekhana Yennam"
date: \today
header-includes:
   - \usepackage{fontspec}
   - \setmainfont{Times New Roman}
   - \usepackage{setspace}
   - \onehalfspacing
   - \usepackage{enumitem}
   - \usepackage{float}
   - \usepackage{url}
   - \usepackage{listings}
   # - \usepackage{biblatex}
   # - \addbibresource{ise560report.bib}
   - \pagenumbering{gobble}
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    fig_caption: true
documentclass: article
fontsize: 12pt
geometry: margin=1in 
urlcolor: blue
---

\begin{figure}[b!]
  \centering
  \includegraphics[width = 0.48\textwidth]{images/ise_logo.pdf}
\end{figure}

\newpage

\pagenumbering{roman}
\setcounter{page}{2}

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage

\pagenumbering{arabic}

\section{Executive Summary}

\section{Introduction}

Lenovo is an international company and a market leader in the personal computer industry.  One tool Lenovo relies upon in assessing customer satisfaction is Net Promoter Score (NPS); this is a standard metric used by major corporations across many industries.  However, latency is a limitation because it can take several months to aggregate sufficient data to calculate NPS.  Therefore our team was tasked with developing a model to predict NPS from more timely information such as customer comments and ratings from websites and telemetry data.

In addition to developing a predictive model for NPS, our team also characterized the natural evolution of both NPS and customer sentiment over time.  These stochastic models form the basis for a Markov Decision Process (MDP) which can be used by Lenovo to identify the optimal actions to take based on the available information.  

Finally, our team analyzed the customer sentiment and comments to identify factors correlated with the highest and lowest satistfaction products.  These results may help identify areas for Lenovo to focus further investments that can improve long-term customer satistifaction. Subsequent sections of this document provide detailed explanations of each phase of our analysis and associated recommendations.

\section{Predictive Model}

Our team considered three approaches to developing a predictive model--linear regression, proportional odds model and time series analysis--because these seemed potentially suitable for this problem and the team had previous experience with these modeling approaches.  We ultimately chose a linear model which will be described in detail here; a summary of the other methods and why we do not recommend them at this time can be found in Appendix \ref{app:PredModel}.

We fit numerous linear models using combinations of the options listed below:
\begin{enumerate}[noitemsep]
  \item Epoch: raw data grouped by week, bi-monthly and monthly
  \item Segment: single segment (commercial and consumer combined) versus separate segements
  \item Predictors: Product Sentiment Index (PSI), average star rating, Product Series, Product Name and interactions between predictors
\end{enumerate}

We then evaluated each candidate model in a two-step process.  First, we examined residuals plots of each model to determine if key assumptions (e.g., normality and homogeneity of variance) of a linear model were satisfied.  Aggregating data weekly or bi-monthly often meant there were very few data points per epoch and/or there would be significant outliers which meant that one or both of the key assumptions were not satisfied.  Therefore we determined aggreating data by month yielded beter results for fitting the linear model.  The second phase of our analysis used F-tests/Analysis of Variance (ANOVA) to identify predictors in the full model (PSI, average stars, Series and Series*PSI interaction) which did not significantly improve the model fit and thus could be removed to yield a more parsimonious model.  Detailed steps in the model selection process can also be found in Appendix \ref{app:PredModel}.  Our final recommendation is two different models for the consumer and commercial segments:

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width="75%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(error = FALSE)

library(tidyverse)
library(lubridate)
library(grid)
library(gridExtra)
# library(kableExtra)
library(tidyverse)
#library(kableExtra)
library(car)
library(caret)

source("../src_lenovo_project.R")
source("../LenovoAnalysis/data_prep.R")
source("../LenovoAnalysis/helper_calcs.R")
source('../LenovoAnalysis/MDP_lenovo_helper.R', encoding = 'UTF-8')
filter_raw_data()

load("../CleanData/filtered_sentiment_data.Rdata")
load("../CleanData/filtered_survey_data.Rdata")
load("../CleanData/filtered_product_lists.Rdata")
load("../CleanData/timeSeries.Rdata")
```

```{r}
# Consumer Only
psi <- sentiment.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_consumer <- inner_join(psi, nps) %>%
  filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Fit model without interaction term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl2 <- train(nps ~ SeriesName + psi + stars, data=all_consumer,
              method = "lm", trControl = train.control)
```

```{r}
# Commercial Only
psi <- sentiment.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_comm <- inner_join(psi, nps) %>%
  #filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Commercial model without stars
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl4 <- train(nps ~ SeriesName*psi, data=all_comm,
              method = "lm", trControl = train.control)
```

$$NPS_{consumer} = (2.43 + \beta_1) - 0.16 * PSI + 13.42 * stars$$
$$NPS_{commercial} = (38.87 + \beta_1) + (\beta_2 - 0.49) * PSI$$

where the $\beta_i$ are listed in Appendix \ref{app:coeff} due to the large number of catgeories in Series.

After model selection, we used k-fold cross validation to estimate the prediction performance of the selected models. Although the model selection process did result in statistically significant coefficients for PSI and stars, the results from the k-fold cross validation show that both models still leave significant room for improvement based on the Root Mean Square Error and Mean Absolute Error (Table \ref{tbl:KFold}).

\begin{table}[ht]
\centering
\caption{Summary of K-fold Cross-Validation Results}
\begin{tabular}{l|c|c|c|}
\cline{2-4}
\multicolumn{1}{c|}{}                  & F-test p-value   & RMSE  & MAE    \\ \hline
\multicolumn{1}{|l|}{Consumer Model}   & \textless{}0.001 & 25.96 & 19.69  \\ \hline
\multicolumn{1}{|l|}{Commercial Model} & \textless{}0.001 & 28.85 & -21.73 \\ \hline
\end{tabular}
\label{tbl:KFold}
\end{table}

It is perhaps easier to understand these results by comparing plots of observed versus predicted.  Figures \ref{fig:mdlConsumer} and \ref{fig:mdlComm} are scatterplots for the Consumer and Commercial data, respectively.  In both sets of plots, the grey points are the observed (NPS, PSI) scores, the grey line is the regression line for the marginal model $NPS = \beta_o + \beta_1 * PSI$ and the red dots are the _predicted values_ from equations (1) and (2) respectively.  While the predicted (red) points follow the general trend of the observed (grey) points, there is still significant variation that is not accounted for by the models.  If Lenovo started archiving the telemetry data, then new models incorporating the telemetry could be fit once several months of data is available; it is entirely possible incorporating telemetry data into the model could reduce the unaccounted for variation and give better predictions.

TO DO: Number equations for linear models

```{r fig.cap="\\label{fig:mdlConsumer}Marginal Model Plots, Consumer NPS vs. PSI"}
# Look at scatterplots fitted vs actual

cbind(all_consumer, 
      pred2=predict(mdl2$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred2), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

```{r fig.cap="\\label{fig:mdlComm}Marginal Model Plots, Commercial NPS vs. PSI"}
# Look at scatterplots fitted vs actual

cbind(all_comm, 
      pred4=predict(mdl4$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred4), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

\section{Markov Decision Process}

The next phase of our analysis developed a Markov decision process (MDP) to identify optimal policies based on customer sentiment or satisfaction.  Where Lenovo did not identify specific componenents (e.g., actions, rewards) necessary for the MDP, we will make assumptions and identify them as such.

\subsection{States}\label{sec:statedef}

We considered using either PSI or predicted NPS  states as the basis for the MDP.  The challenge with using PSI is that defining states is somewhat arbitrary.  We considered standardizing the PSI scores and then delineating states using multiples of the standard deviation.  However, we felt that approach would not be as intuitive to understand because the states would not be defined according to the [-100, 100] scale Lenovo is familiar with for PSI.  On the other hand, there are established industry-standard categories for NPS\cite{Nps19}.  Therefore, we chose to use these industry-standards as the basis for the predicted NPS states in our MDP, and they are listed below.

\begin{itemize}[noitemsep]
  \item Very Low (--): $NPS \le 8$ 
  \item Moderately Low (-): $8 < NPS \le 35$ 
  \item Normal (0): $35 < NPS \le 45$ 
  \item Moderately High (+): $45 < NPS \le 65$ 
  \item Very High (++): $65 < NPS$ 
\end{itemize}

We will include telemetry status in the MDP since Lenovo would like to make use of telemetry in the future.  We will assume there are two possible telemetry states, Normal (N) and Abnormal (A).  See Appendix \ref{app:tm} for an example of how Normal/Abnormal states can be characterized once Lenovo has archived several months worth of data.

Therefore the possible combined states for the MDP are {(++,N), (++,A), (+,N), (+,A), (0,N), (0,A), (-,N), (-,A), (--,N), (--,A)}

\subsection{Actions}

We assume two possible actions: {Do Nothing, Conduct Root Cause Analysis}.  Further, we will assume that a root cause analysis would only be considered if the NPS state is Very Low or Moderately Low.

\begin{table}[h!]
\centering
\caption{Example Actions}
\begin{tabular}{lc|c|c|c|c|c|}
\cline{3-7}
\multicolumn{1}{c}{}     &                & \multicolumn{5}{c|}{Applicable NPS States}  \\ \hline
\multicolumn{1}{|c|}{ID} & Action         & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{1}  & Do Nothing                  & Y  & Y & Y      & Y  & Y      \\ \hline
\multicolumn{1}{|l|}{2}  & Conduct Root Cause Analysis & Y  & Y & N      & N  & N       \\ \hline
\end{tabular}
\end{table}

\subsection{Transitions}\label{sec:transitions}

We calculated the natural evolution of NPS by month (i.e., same epoch as linear model) from the survey data.  Since we do not have enough data to calculate the natural evolution of Telemetry states, we will assume a transition matrix as shown below.

\begin{table}[h]
\caption{Individual Transition Matrices, NPS and Telemetry}
\centering
\hspace*{\fill}
\begin{tabular}[t]{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & - - & - & 0 & + & ++ \\ \hline
\multicolumn{1}{|l|}{- -} & 0.451    & 0.220    & 0.055   & 0.143 & 0.132    \\ \hline
\multicolumn{1}{|l|}{-} & 0202    & 0.506    & 0.079   & 0.101  & 0.111       \\ \hline
\multicolumn{1}{|l|}{0}   & 0.161   & 0.355  & 0.194     & 0.160   & 0.097       \\ \hline
\multicolumn{1}{|l|}{+} & 0.200  & 0.220    & 0.160     & 0.200  & 0.220       \\ \hline
\multicolumn{1}{|l|}{++} & 0.214    & 0.054    & 0.125     & 0.161    & 0.446      \\ \hline
\end{tabular}
\hfill
\begin{tabular}[t]{l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{}      & N & A  \\ \hline
\multicolumn{1}{|l|}{N}      & 0.75 & 0.25  \\ \hline
\multicolumn{1}{|l|}{A}      & 0.25 & 0.75  \\ \hline
\end{tabular}
\hspace*{\fill}
\label{tbl:P1}
\end{table}

We combined the individual transition matrices to form the transition matrix for our MDP.  This natural evolution for the combined states is equivalent to the "Do Nothing" action.

```{r}

tmp <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  calcNPS()

# Define NPS categories
VERY_HIGH <- 60.0
MOD_HIGH <- 45.0
NORMAL <- 35.0
MOD_LOW <- 8.0

res <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  NPSTransitions(VERY_HIGH, MOD_HIGH, NORMAL, MOD_LOW)

p.s = as.matrix(res$P[,2:6])

# Define telemetry matrix
p.tel = matrix(c(0.75, 0.25, 0.25, 0.75),nrow = 2, ncol = 2, byrow = TRUE)
rownames(p.tel) <- c("N","A")
colnames(p.tel) <- c("N","A")

P1 = combine.matrix(p.s,
                    p.tel)


rownames(P1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")
colnames(P1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")

knitr::kable(round(P1, digits=3), row.names=TRUE, caption = "Consumer, All States, Action = Do Nothing  \\label{tbl:P1}")

```

\newpage
Next, we assume the action "Conduct Root Cause Analysis" reduces the probability of transitioning to Very_Low by $100p\%$ compared to "Do Nothing" if Telemetry is Abnormal and $75p\%$ if Telemetry is Normal (i.e., root cause analysis is more likely to succeed if there is abnormal telemetry data to troublehsoot).  The change in percentage is evenly distributed among the remaining 4 states.  For example, if $p=0.1$ then the transition matrix for ``Conduct Root Cause Analysis" is shown in Table \ref{tbl:P2}.  Note that only the rows which differ from Table \ref{tbl:P1} are displayed.


```{r}

d <- 0.75
p <- 0.1
n <- 5
P2 <- P1

# Modify VERY_LOW, Normal Telemetry
delta <- d * p * P1[1,1]/sum(P1[1,1:5])
P2[1,1] <- P1[1,1] - delta
P2[1,2] <- P1[1,2] + delta/(n-1)
P2[1,3] <- P1[1,3] + delta/(n-1)
P2[1,4] <- P1[1,4] + delta/(n-1)
P2[1,5] <- P1[1,5] + delta/(n-1)
delta <- d * p * P1[1,6]/sum(P1[1,6:10])
P2[1,6] <- P1[1,6] - delta
P2[1,7] <- P1[1,7] + delta/(n-1)
P2[1,8] <- P1[1,8] + delta/(n-1)
P2[1,9] <- P1[1,9] + delta/(n-1)
P2[1,10] <- P1[1,10] + delta/(n-1)

# Modify MOD_LOW, Normal Telemetry
delta <- d *p * P1[2,1]/sum(P1[2,1:5])
P2[2,1] <- P1[2,1] - delta
P2[2,2] <- P1[2,2] + delta/(n-1)
P2[2,3] <- P1[2,3] + delta/(n-1)
P2[2,4] <- P1[2,4] + delta/(n-1)
P2[2,5] <- P1[2,5] + delta/(n-1)
delta <- d *p * P1[2,6]/sum(P1[2,6:10])
P2[2,6] <- P1[2,6] - delta
P2[2,7] <- P1[2,7] + delta/(n-1)
P2[2,8] <- P1[2,8] + delta/(n-1)
P2[2,9] <- P1[2,9] + delta/(n-1)
P2[2,10] <- P1[2,10] + delta/(n-1)

# Modify VERY_LOW, Abnormal Telemetry
delta <- p * P1[6,1]/sum(P1[6,1:5])
P2[6,1] <- P1[6,1] - delta
P2[6,2] <- P1[6,2] + delta/(n-1)
P2[6,3] <- P1[6,3] + delta/(n-1)
P2[6,4] <- P1[6,4] + delta/(n-1)
P2[6,5] <- P1[6,5] + delta/(n-1)
delta <- p * P1[6,6]/sum(P1[6,6:10])
P2[6,6] <- P1[6,6] - delta
P2[6,7] <- P1[6,7] + delta/(n-1)
P2[6,8] <- P1[6,8] + delta/(n-1)
P2[6,9] <- P1[6,9] + delta/(n-1)
P2[6,10] <- P1[6,10] + delta/(n-1)

# Modify MOD_LOW, Normal Telemetry
delta <- p * P1[7,1]/sum(P1[7,1:5])
P2[7,1] <- P1[7,1] - delta
P2[7,2] <- P1[7,2] + delta/(n-1)
P2[7,3] <- P1[7,3] + delta/(n-1)
P2[7,4] <- P1[7,4] + delta/(n-1)
P2[7,5] <- P1[7,5] + delta/(n-1)
delta <- p * P1[7,1]/sum(P1[7,6:10])
P2[7,6] <- P1[7,6] - delta
P2[7,7] <- P1[7,7] + delta/(n-1)
P2[7,8] <- P1[7,8] + delta/(n-1)
P2[7,9] <- P1[7,9] + delta/(n-1)
P2[7,10] <- P1[7,10] + delta/(n-1)

knitr::kable(round(P2[c(1,2,6,7),], digits=3), row.names=TRUE, 
             caption = "Consumer, All States, Action = Root Cause Analysis \\label{tbl:P2}")

```

\subsection{Rewards}

We assume rewards are of the form shown in Tables \ref{tbl:R1} and \ref{tbl:R2} with parameters $r1$ and $r2$.

\begin{table}[h!]
\caption{Reward Matrix Templates}
\centering
\hspace*{\fill}
\begin{tabular}[t]{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & - - & - & 0 & + & ++ \\ \hline
\multicolumn{1}{|l|}{- -} & -2r1    & -r1    & 0   & r1 & 2r1    \\ \hline
\multicolumn{1}{|l|}{-} & -2r1    & -r1    & 0   & r1  & 2r1       \\ \hline
\multicolumn{1}{|l|}{0}   & -2r1   & -r1  & 0     & r1   & 2r1       \\ \hline
\multicolumn{1}{|l|}{+} & -2r1  & -r1    & 0     & r1  & 2r1       \\ \hline
\multicolumn{1}{|l|}{++} & -2r1    & -r1    & 0     & r1    & 2r1      \\ \hline
\end{tabular}
\hfill
\begin{tabular}[t]{l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{}      & N & A  \\ \hline
\multicolumn{1}{|l|}{N}      & 2r2 & -r2  \\ \hline
\multicolumn{1}{|l|}{A}      & r2 & -2r2  \\ \hline
\end{tabular}
\hspace*{\fill}
\label{tbl:R1}
\end{table}

If we assume $r1=10$, $r2=5$ and conducting a root cause analysis has cost $c=5$.  The resulting Rewards Matrices are shown in Tables \ref{tbl:Reward2} and \ref{tbl:Reward3}.


\subsection{Result}

Given the previous assumptions, we analyzed the MDP for $p \in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]$ and a discount factor of $0.95$.  The results are listed in Table \ref{tbl:MDP}.  Conducting the root cause analysis is optimal _only_ if the NPS score is Very Low _and_ it is expected to reduce the probability of staying in the Very_Low state by 40% or more.  For all other conditions and states, the optimal policy is Do Nothing.   The R source code used to conduct this analysis is listed in Appendix \ref{app:MDP} and the values for reward, cost, discount factor and $p$ are parameters which can be easily modified by Lenovo.

\begin{table}[h!]
\caption{Optimal Policy as a Function of p}
\centering
\begin{tabular}{l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{}      & $p<0.4$ & $p \ge 0.4$ \\ \hline
\multicolumn{1}{|l|}{Very Low}      & Do Nothing & Root Cause Analysis \\ \hline
\multicolumn{1}{|l|}{Mod. Low}      & Do Nothing & Do Nothing \\ \hline
\multicolumn{1}{|l|}{Normal}      & Do Nothing & Do Nothing \\ \hline
\multicolumn{1}{|l|}{Mod. High}      & Do Nothing & Do Nothing \\ \hline
\multicolumn{1}{|l|}{Very High}      & Do Nothing & Do Nothing \\ \hline
\end{tabular}
\label{tbl:MDP}
\end{table}

\section{Customer Sentiment Analysis}

\section{Recommendations}
Our recommendations are as follows:

\clearpage

\bibliographystyle{unsrt}
\bibliography{ise560report}
<!-- \printbibliography -->

\clearpage

\appendix

\section{Coefficients for Recommended Linear Models}\label{app:coeff}


```{r}
# Consumer
beta_coeff <- round(mdl2$finalModel$coefficients[2:14], digits=2)
nconsumer <- sub("SeriesName", "", names(beta_coeff))
knitr::kable(data.frame(nconsumer, unname(beta_coeff)),
             col.names = c("Consumer Series", "$\\beta_1$"),
             caption = "Coefficients for Consumer Model") 

# Commercial
beta_coeff1 <- round(mdl4$finalModel$coefficients[2:12], digits=2)
beta_coeff2 <- round(mdl4$finalModel$coefficients[14:24], digits=2)
ncomm <- sub("SeriesName", "", names(beta_coeff1))
knitr::kable(data.frame(ncomm, unname(beta_coeff1), unname(beta_coeff2)), 
             col.names = c("Commercial Series", "$\\beta_1$", "$\\beta_2$"),
             caption = "Coefficients for Commercial Model") 

```

\clearpage

\section{Predictive Model Approaches}\label{app:PredModel}

\subsection{Linear Model}

\subsection{Proportional Odds Model}

\subsection{Time Series Analysis}

Considering the pNPS score as a stochastic process where the random variables are realizations of pNPS value, attempts to perform classical  time-series analysis.  If $X_t$ denoted pNPS at time $t$, the auto-regressive moving-average(ARMA) model \cite{box2015time} would be

$$X_{t} = \sum_{i = 1}^p \alpha_i X_{t-i} + \sum_{j = 1}^q  \beta_j \theta_{t-j} $$.

where $\alpha_i$ $\forall i \in \{1,2,\ldots, p\}$  and $\beta_j$ $\forall j \in \{1,2,\ldots, q\}$ are coefficients of the linear model making a 1-step prediction of pNPS,  $\theta_t$ is the moving average component at time $t$ following a normal distribution with zero mean and variance $\sigma^2_w$. As the pNPS scores are heteroscedastic, the time series are also differenced $d$ times to satisfy the condition of stationarity.  Note that $p$, $d$ and $q$ are  referred to as the order of the model.  In Tables \ref{tab:ARIMA_Commercial} and \ref{tab:ARIMA_Consumer}, we present train set errors of best ARIMA model with the lowest Akaike information criterion (AIC) with order $(p,q,r)$.  The pNPS calcualtions were made over a period of seven days.  It is also observed that temporal correlation is sensitive to the series.

```{r}
series.ts.main.com <- series.ts.main.com%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0,SeriesName != "ACCESSORIES", SeriesName != "ANNIVERSARY")
series.ts.main <- series.ts.main%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0, SeriesName != "SMART HOME", SeriesName != "OTHER")
knitr::kable(series.ts.main.com[series.ts.main.com$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Commercial", caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))
knitr::kable(series.ts.main[series.ts.main$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Consumer",caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))


```


While the intention is analyze the statistical relationship between adjacent samples of pNPS calculations, the epoch overwhich the pNPS scores are varied from 1 to 30 days.  Figures \ref{fig:arima.comm} and \ref{fig:arima.con} shows the Mean Absolute Errors as a funtion of the epoch over which the pNPS score was calculated.  It is observed for a majority of the Series, the surveys are least correlated with a two-week window.

```{r figs, echo = FALSE,  fig.cap = "\\label{fig:arima.comm} Train set pNPS prediction errors vs window size for commercial data.", fig.width = 8}
plt.com <- ggplot(series.ts.main.com, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.com
```

```{r figs_con, echo = FALSE, fig.cap="\\label{fig:arima.con} Train set pNPS prediction errors vs window size for consumer data.", fig.width = 8}
plt.con <- ggplot(series.ts.main, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.con
```


\section{General}

## Combined State Transtion Matrix

The state transition matrix for the consumer business group is given by

```{r eval=TRUE, fig.cap = "\\label{tab:combined_transition_matrix} A sample combined Telemetry-Survey State transition matrix."}
# Calcualtion of combined state probability matrix

knitr::kable(P1,  row.names = TRUE)  


```

## pNPS Transition matrix after an intervention

```{r}
knitr::kable(P2, row.names=TRUE, caption = "\\label{tab:action1_prob1} Consumer, Software Patch")

```

\clearpage

\section{MDP Source Code}\label{app:MDP}

\lstinputlisting{../LenovoAnalysis/MDP_final.R}

\clearpage

\section{Additional R code}

\subsection{data\_prep.R}

\lstinputlisting{../LenovoAnalysis/data_prep.R}

\subsection{helper\_calcs.R}

\lstinputlisting{../LenovoAnalysis/helper_calcs.R}

\clearpage

\section{Estimating Telemetry States}\label{app:TM}

Assuming a time-series of driver health is available for a particular driver, states can be defined between normal $N$ and abnormal $AN$ operations depending on a performance score $ c_t $ at time $t$ and is given in Equation.\ref{crash_rate_score}.
\begin{equation}\label{crash_rate_score}
c_{t} = min\left\{1,\frac{\text{\# Crashes}}{\text{Total machines }\times \text{ Percent Impacted}}\right\}
\end{equation}
Note that our results would be more relevant if we have access to the number of unique machines that have experienced a crash.  This is can defined as 
$$c_t = \frac{\text{ Number of unique machines crashing at time $t$ }}{\text{Total Number of Machines Impacted at time $t$}}$$

When the performance score is above a threshold, we flag the observations as Anomalous. The transition matrix for telemetry can be constructed similar to the pNPS transition matrix by observing the number of transitions from each state for a given driver update.   specific driver updates (Two states $S_T \in \{N, AN\}$) and is referred to as $P_{T}$.  For the sake of demonstration, we assume the following transition matrix for telemetry:

```{r, echo=FALSE, eval=TRUE}

n.tel = 2
p.tel = matrix(c(0.75, 0.25, 0.25, 0.75),nrow = 2, ncol = 2, byrow = TRUE)
rownames(p.tel) <- c("N","AN")
colnames(p.tel) <- c("N","AN")
knitr::kable(p.tel ,  caption = "A possible telemetry transtion matrix from ",  row.names = TRUE)

```

The final state matrix $P$ would be a combination of $P_S$ and $P_T$ as shown in Section \ref{sec:transitions}.


