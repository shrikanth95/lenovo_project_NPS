---
title: "ISE 560 Project Report"
author: "Shawn Markham, Melissa Wong, Shrikanth Yadav & Lekhana Yennam"
date: \today
header-includes:
   - \usepackage{fontspec}
   - \setmainfont{Times New Roman}
   - \usepackage{setspace}
   - \onehalfspacing
   - \usepackage{enumitem}
   - \usepackage{float}
   - \usepackage{url}
   - \usepackage{listings}
   # - \usepackage{biblatex}
   # - \addbibresource{ise560report.bib}
   - \pagenumbering{gobble}
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    fig_caption: true
documentclass: article
fontsize: 12pt
geometry: margin=1in 
urlcolor: blue
---

\begin{figure}[b!]
  \centering
  \includegraphics[width = 0.48\textwidth]{images/ise_logo.pdf}
\end{figure}

\newpage

\pagenumbering{roman}
\setcounter{page}{2}

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage

\pagenumbering{arabic}

\section{Executive Summary}

\section{Introduction}

Lenovo is an international company and a market leader in the personal computer industry.  One tool Lenovo relies upon in assessing customer satisfaction is Net Promoter Score (NPS); this is a standard metric used by major corporations across many industries.  However, latency is a limitation because it can take several months to aggregate sufficient data to calculate NPS.  Therefore our team was tasked with developing a model to predict NPS from more timely information such as customer comments and ratings from websites and telemetry data.

In addition to developing a predictive model for NPS, our team also characterized the natural evolution of both NPS and customer sentiment over time.  These stochastic models form the basis for a Markov Decision Process (MDP) which can be used by Lenovo to identify the optimal actions to take based on the available information.  

Finally, our team analyzed the customer sentiment and comments to identify factors correlated with the highest and lowest satistfaction products.  These results may help identify areas for Lenovo to focus further investments that can improve long-term customer satistifaction. Subsequent sections of this document provide detailed explanations of each phase of our analysis and associated recommendations.

\section{Predictive Model}

Our team considered three approaches to developing a predictive model--linear regression, proportional odds model and time series analysis--because these seemed potentially suitable for this problem and the team had previous experience with these modeling approaches.  We ultimately chose a linear model which will be described in detail here; a summary of the other methods and why we do not recommend them at this time can be found in Appendix \ref{app:PredModel}.

We fit numerous linear models using combinations of the options listed below:
\begin{enumerate}[noitemsep]
  \item Epoch: raw data grouped by week, bi-monthly and monthly
  \item Segment: single segment (commercial and consumer combined) versus separate segements
  \item Predictors: Product Sentiment Index (PSI), average star rating, Product Series, Product Name and interactions between predictors
\end{enumerate}

We then evaluated each candidate model in a two-step process.  First, we examined residuals plots of each model to determine if key assumptions (e.g., normality and homogeneity of variance) of a linear model were satisfied.  Aggregating data weekly or bi-monthly often meant there were very few data points per epoch and/or there would be significant outliers which meant that one or both of the key assumptions were not satisfied.  Therefore we determined aggreating data by month yielded beter results for fitting the linear model.  The second phase of our analysis used F-tests/Analysis of Variance (ANOVA) to identify predictors in the full model (PSI, average stars, Series and Series*PSI interaction) which did not significantly improve the model fit and thus could be removed to yield a more parsimonious model.  Detailed steps in the model selection process can also be found in Appendix \ref{app:PredModel}.  Our final recommendation is two different models for the consumer and commercial segments:

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# rm(list=ls())

knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width="75%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(error = FALSE)

library(tidyverse)
library(lubridate)
library(grid)
library(gridExtra)
# library(kableExtra)
library(tidyverse)
#library(kableExtra)
library(car)
library(caret)

source("../src_lenovo_project.R")
source("../LenovoAnalysis/data_prep.R")
source("../LenovoAnalysis/helper_calcs.R")
source('../LenovoAnalysis/MDP_lenovo_helper.R', encoding = 'UTF-8')
filter_raw_data()

load("../CleanData/filtered_sentiment_data.Rdata")
load("../CleanData/filtered_survey_data.Rdata")
load("../CleanData/filtered_product_lists.Rdata")
load("../CleanData/timeSeries.Rdata")
load("../CleanData/correlation.Rdata")
```

```{r}
# Consumer Only
psi <- sentiment.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.consumer.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_consumer <- inner_join(psi, nps) %>%
  filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Fit model without interaction term
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl2 <- train(nps ~ SeriesName + psi + stars, data=all_consumer,
              method = "lm", trControl = train.control)
```

```{r}
# Commercial Only
psi <- sentiment.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Comment.Date, format = "%m/%d/%y"), unit = "month")) %>%
  calcPSI()

nps <- survey.comm.all %>%
  group_by(SeriesName,
           date = floor_date(as.Date(Date.Survey, format = "%m/%d/%Y"), unit = "month")) %>%
  calcNPS()

# Combine to get data.frame with nps and psi matching by date and Series
# Drop any Series with less than 3 data points (insufficient data)
all_comm <- inner_join(psi, nps) %>%
  #filter(total > 2) %>%
  mutate(occurs = n()) %>%
  filter(occurs > 3) %>%
  drop_na()

# Commercial model without stars
train.control <- trainControl(method = "repeatedcv", number = 10, repeats=3)

mdl4 <- train(nps ~ SeriesName*psi, data=all_comm,
              method = "lm", trControl = train.control)
```

$$NPS_{consumer} = (2.43 + \beta_1) - 0.16 * PSI + 13.42 * stars$$
$$NPS_{commercial} = (38.87 + \beta_1) + (\beta_2 - 0.49) * PSI$$

where the $\beta_i$ are listed in Appendix \ref{app:coeff} due to the large number of catgeories in Series.

After model selection, we used k-fold cross validation to estimate the prediction performance of the selected models. Although the model selection process did result in statistically significant coefficients for PSI and stars, the results from the k-fold cross validation show that both models still leave significant room for improvement based on the Root Mean Square Error and Mean Absolute Error (Table \ref{tbl:KFold}).

\begin{table}[ht]
\centering
\caption{Summary of K-fold Cross-Validation Results}
\begin{tabular}{l|c|c|c|}
\cline{2-4}
\multicolumn{1}{c|}{}                  & F-test p-value   & RMSE  & MAE    \\ \hline
\multicolumn{1}{|l|}{Consumer Model}   & \textless{}0.001 & 25.96 & 19.69  \\ \hline
\multicolumn{1}{|l|}{Commercial Model} & \textless{}0.001 & 28.85 & -21.73 \\ \hline
\end{tabular}
\label{tbl:KFold}
\end{table}

It is perhaps easier to understand these results by comparing plots of observed versus predicted.  Figures \ref{fig:mdlConsumer} and \ref{fig:mdlComm} are scatterplots for the Consumer and Commercial data, respectively.  In both sets of plots, the grey points are the observed (NPS, PSI) scores, the grey line is the regression line for the marginal model $NPS = \beta_o + \beta_1 * PSI$ and the red dots are the _predicted values_ from equations (1) and (2) respectively.  While the predicted (red) points follow the general trend of the observed (grey) points, there is still significant variation that is not accounted for by the models.  If Lenovo started archiving the telemetry data, then new models incorporating the telemetry could be fit once several months of data is available; it is entirely possible incorporating telemetry data into the model could reduce the unaccounted for variation and give better predictions.

TO DO: Number equations for linear models

```{r fig.cap="\\label{fig:mdlConsumer}Marginal Model Plots, Consumer NPS vs. PSI"}
# Look at scatterplots fitted vs actual

cbind(all_consumer, 
      pred2=predict(mdl2$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred2), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

```{r fig.cap="\\label{fig:mdlComm}Marginal Model Plots, Commercial NPS vs. PSI"}
# Look at scatterplots fitted vs actual

cbind(all_comm, 
      pred4=predict(mdl4$finalModel)) %>%
ggplot(aes(x=psi, y=nps, group=SeriesName)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", se=FALSE, color="darkgrey") +
  geom_point(aes(y=pred4), alpha=0.3, color="red") +
  facet_wrap(~SeriesName)
```

\section{Markov Decision Process}

The next phase of our analysis developed a Markov decision process (MDP) to identify optimal policies based on customer sentiment or satisfaction.  Where Lenovo did not identify specific componenents (e.g., actions, rewards) necessary for the MDP, we will make assumptions and identify them as such.

\subsection{States}\label{sec:statedef}

We considered using either PSI or predicted NPS  states as the basis for the MDP.  The challenge with using PSI is that defining states is somewhat arbitrary.  We considered standardizing the PSI scores and then delineating states using multiples of the standard deviation.  However, we felt that approach would not be as intuitive to understand because the states would not be defined according to the [-100, 100] scale Lenovo is familiar with for PSI.  On the other hand, there are established industry-standard categories for NPS\cite{Nps19}.  Therefore, we chose to use these industry-standards as the basis for the predicted NPS states in our MDP, and they are listed below.

\begin{itemize}[noitemsep]
  \item Very Low (--): $NPS \le 8$ 
  \item Moderately Low (-): $8 < NPS \le 35$ 
  \item Normal (0): $35 < NPS \le 45$ 
  \item Moderately High (+): $45 < NPS \le 65$ 
  \item Very High (++): $65 < NPS$ 
\end{itemize}

We will include telemetry status in the MDP since Lenovo would like to make use of telemetry in the future.  We will assume there are two possible telemetry states, Normal (N) and Abnormal (A).  See Appendix \ref{app:tm} for an example of how Normal/Abnormal states can be characterized once Lenovo has archived several months worth of data.

Therefore the possible combined states for the MDP are {(++,N), (++,A), (+,N), (+,A), (0,N), (0,A), (-,N), (-,A), (--,N), (--,A)}

\subsection{Actions}

We assume two possible actions: {Do Nothing, Conduct Root Cause Analysis}.  Further, we will assume that a root cause analysis would only be considered if the NPS state is Very Low or Moderately Low.

\begin{table}[h!]
\centering
\caption{Example Actions}
\begin{tabular}{lc|c|c|c|c|c|}
\cline{3-7}
\multicolumn{1}{c}{}     &                & \multicolumn{5}{c|}{Applicable NPS States}  \\ \hline
\multicolumn{1}{|c|}{ID} & Action         & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{1}  & Do Nothing                  & Y  & Y & Y      & Y  & Y      \\ \hline
\multicolumn{1}{|l|}{2}  & Conduct Root Cause Analysis & Y  & Y & N      & N  & N       \\ \hline
\end{tabular}
\end{table}

\subsection{Transitions}

We then calculated the natural evolution of NPS by month (i.e., same epoch as linear model) from the survey data.

\begin{table}[h!]
\caption{Consumer, NPS Only, Action = Do Nothing}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & - - & - & 0 & + & ++ \\ \hline
\multicolumn{1}{|l|}{- -} & 0.451    & 0.220    & 0.055   & 0.143 & 0.132    \\ \hline
\multicolumn{1}{|l|}{-} & 0202    & 0.506    & 0.079   & 0.101  & 0.111       \\ \hline
\multicolumn{1}{|l|}{0}   & 0.161   & 0.355  & 0.194     & 0.160   & 0.097       \\ \hline
\multicolumn{1}{|l|}{+} & 0.200  & 0.220    & 0.160     & 0.200  & 0.220       \\ \hline
\multicolumn{1}{|l|}{++} & 0.214    & 0.054    & 0.125     & 0.161    & 0.446      \\ \hline
\end{tabular}
\label{tbl:P1}
\end{table}

Since we do not have enough data to calculate the natural evolution of Telemetry states, we will assume the following:

\begin{table}[h!]
\caption{Consumer, Telemetry Only, Action = Do Nothing}
\centering
\begin{tabular}{l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{}      & N & A  \\ \hline
\multicolumn{1}{|l|}{N}      & 0.75 & 0.25  \\ \hline
\multicolumn{1}{|l|}{A}      & 0.25 & 0.75  \\ \hline
\end{tabular}
\end{table}

We then combine the individual transition matrices to form the transition matrix for our MDP.  This natural evolution for the combined states is equivalent to the "Do Nothing" action.

```{r}

tmp <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  calcNPS()

# Define NPS categories
VERY_HIGH <- 60.0
MOD_HIGH <- 45.0
NORMAL <- 35.0
MOD_LOW <- 8.0

res <- survey.consumer.all %>%
  group_by(SeriesName,
           time = floor_date(as.Date(Date.Survey, format = "%m/%d/%y"), unit = "month")) %>%
  NPSTransitions(VERY_HIGH, MOD_HIGH, NORMAL, MOD_LOW)

p.s = as.matrix(res$P[,2:6])

# Define telemetry matrix
p.tel = matrix(c(0.75, 0.25, 0.25, 0.75),nrow = 2, ncol = 2, byrow = TRUE)
rownames(p.tel) <- c("N","A")
colnames(p.tel) <- c("N","A")

P1 = combine.matrix(p.s,
                    p.tel)


rownames(P1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")
colnames(P1) <- c("- -, N"," -, N"," 0, N"," +, N","++, N","- - , A"," -, A"," 0, A"," +, A","++, A")

knitr::kable(round(P1, digits=3), row.names=TRUE, caption = "Consumer, All States, Action = Do Nothing")

```

Next, we assume the action "Conduct Root Cause Analysis" reduces the probability of transitioning to Very_Low by $100p\%$ as compared to "Do Nothing", and that percentage is evenly distributed among the remaining 4 states.  For example, if $p=0.1$ then the transition matrix for ``Conduct Root Cause Analysis" is shown in Table \ref{tbl:P2}.

\begin{table}[h!]
\caption{Consumer, Action = Conduct Root Cause Analysis}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & 0.406    & 0.231    & 0.066   & 0.154 & 0.143    \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & 0.182    & 0.511    & 0.084   & 0.106  & 0.117       \\ \hline
\multicolumn{1}{|l|}{Normal}   & 0.161   & 0.355  & 0.194     & 0.160   & 0.097       \\ \hline
\multicolumn{1}{|l|}{Mod High} & 0.200  & 0.220    & 0.160     & 0.200  & 0.220       \\ \hline
\multicolumn{1}{|l|}{Very High} & 0.214    & 0.054    & 0.125     & 0.161    & 0.446      \\ \hline
\end{tabular}
\label{tbl:P2}
\end{table}

\subsection{Rewards}

We assume rewards are of the form shown in Table \ref{tbl:Reward1} with a parameter $r$.

\begin{table}[h!]
\caption{Reward Matrix Template}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & -2r      & -r       & 0      & r         & 2r        \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & -2r      & -r       & 0      & r         & 2r        \\ \hline
\multicolumn{1}{|l|}{Normal}   & -2r      & -r      & 0      & r          & 2r        \\ \hline
\multicolumn{1}{|l|}{Mod High} & -2r      & -r      & 0      & r          & 2r        \\ \hline
\multicolumn{1}{|l|}{Very High} & -2r      & -r     & 0      & r          & 2r        \\ \hline
\end{tabular}
\label{tbl:Reward1}
\end{table}

Next, we assume $r=10$ and conducting a root cause analysis has cost $c=5$.  The resulting Rewards Matrices are shown in Tables \ref{tbl:Reward2} and \ref{tbl:Reward3}.

\begin{table}[h!]
\caption{Example Reward Matrix- Do Nothing}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}     & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & -20      & -10       & 0      & 10         & 20        \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & -20      & -10       & 0      & 10         & 20        \\ \hline
\multicolumn{1}{|l|}{Normal}   & -20      & -10      & 0      & 10          & 20        \\ \hline
\multicolumn{1}{|l|}{Mod High} & -20      & -10      & 0      & 10          & 20        \\ \hline
\multicolumn{1}{|l|}{Very High} & -20      & -10     & 0      & 10          & 20        \\ \hline
\end{tabular}
\label{tbl:Reward2}
\end{table}

\begin{table}[h!]
\caption{Example Reward Matrix- Conduct Root Cause Analysis}
\centering
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
\multicolumn{1}{c|}{}      & Very Low & Mod. Low & Normal & Mod. High & Very High \\ \hline
\multicolumn{1}{|l|}{Very Low} & -25      & -15       & -5      & 5         & 15        \\ \hline
\multicolumn{1}{|l|}{Mod. Low} & -25      & -15       & -5      & 5         & 15        \\ \hline
\multicolumn{1}{|l|}{Normal}   & -      & -      & -      & -          & -        \\ \hline
\multicolumn{1}{|l|}{Mod High} & -      & -      & -      & -          & -        \\ \hline
\multicolumn{1}{|l|}{Very High} & -      & -     & -      & -          & -        \\ \hline
\end{tabular}
\label{tbl:Reward3}
\end{table}

\subsection{Result}

Given the previous assumptions, we analyzed the MDP for $p \in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]$ and a discount factor of $0.95$.  The results are listed in Table \ref{tbl:MDP}.  Conducting the root cause analysis is optimal _only_ if the NPS score is Very Low _and_ it is expected to reduce the probability of staying in the Very_Low state by 40% or more.  For all other conditions and states, the optimal policy is Do Nothing.   The R source code used to conduct this analysis is listed in Appendix \ref{app:MDP} and the values for reward, cost, discount factor and $p$ are parameters which can be easily modified by Lenovo.

\begin{table}[h!]
\caption{Optimal Policy as a Function of p}
\centering
\begin{tabular}{l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{}      & $p<0.4$ & $p \ge 0.4$ \\ \hline
\multicolumn{1}{|l|}{Very Low}      & Do Nothing & Root Cause Analysis \\ \hline
\multicolumn{1}{|l|}{Mod. Low}      & Do Nothing & Do Nothing \\ \hline
\multicolumn{1}{|l|}{Normal}      & Do Nothing & Do Nothing \\ \hline
\multicolumn{1}{|l|}{Mod. High}      & Do Nothing & Do Nothing \\ \hline
\multicolumn{1}{|l|}{Very High}      & Do Nothing & Do Nothing \\ \hline
\end{tabular}
\label{tbl:MDP}
\end{table}

\section{Recommendations using Customer Sentiment Analysis}

In order to identify areas that are faring well with the population and those that need improvement, it is useful to tap into large datasets for information on potential areas of improvemnt.  We analyze the categories in the sentiment data as a proxy for pNPS scores.  

First, we start by observing the temporal correlation $\rho$ between PSI scores and pNPS when the former is conditioned on both categories and series.  It is observed that the PSI associated with categories listed in Fig.\ref{fig:neg-corr} positively correlate with pNPS timeseries.  Table \ref{tab:corr_category}, highlights the number of categories that correlate non-trivially for each series in the consumer group.  Each PSI/pNPS score was calcualted over a period of 1 month.  Assuming pNPS scores are true representations of the user perception, understanding correlation with PSI helps in setting a baseline for comparing againts categories that need improvement.  Categories that correlate negatively with pNPS scores can be considred as false postive indicators.

```{r}
overall.study <- merge(summary.cat, psi.study,by = "Category")
neg.psi <- overall.study[order(overall.study$psi.all),c('Category', 'SeriesName', 'correlation',"psi.all")] %>% filter(correlation!=0&psi.all<8)

overall.study2 <- merge(summary.cat2, psi.study,by = "Category")
neg.psi2 <- overall.study2[order(overall.study2$psi.all),c('Category', 'SeriesName', 'correlation',"psi.all")] %>% filter(correlation!=0&psi.all<8)

```

Second, we isolate on specific categories and observe the PSI scores in the $very$ $low$ state over the entire dataset in the Appendix in Table\ref{tab:neg.psi}.  The final list of categories that needs to addressed can be deduced by sorting Table\ref{tab:neg.psi} based on the correlation. The final list of categories that would need improvements are given in Table.\ref{tab:improvements}.  A similar analysis is done for Commercial series and the suggested improvements are given in Table.\ref{tab:improvements2}.

```{r}
imp <- neg.psi[order(neg.psi$correlation,decreasing = TRUE),] %>% filter(correlation > 0.5) %>% mutate(correlation = round(correlation,digits = 2),
                                                                                                       psi.all = round(psi.all, digits = 2))

knitr::kable(imp,col.names =  c("Category","Series", "$\\rho$","PSI"), caption = "\\label{tab:improvements} Categories in Consumer Series that need improvements")

imp2 <- neg.psi2[order(neg.psi2$correlation,decreasing = TRUE),] %>% filter(correlation > 0.5) %>% mutate(correlation = round(correlation,digits = 2),
                                                                                                       psi.all = round(psi.all, digits = 2))
knitr::kable(imp2,col.names =  c("Category","Series", "$\\rho$","PSI"), caption = "\\label{tab:improvements2} Categories in Commercial Series that need improvements")

```





\section{Conclusions}
Our recommendations are as follows:

\clearpage

\bibliographystyle{unsrt}
\bibliography{ise560report}
<!-- \printbibliography -->

\clearpage

\appendix

\section{Coefficients for Recommended Linear Models}\label{app:coeff}


```{r}
# Consumer
beta_coeff <- round(mdl2$finalModel$coefficients[2:14], digits=2)
nconsumer <- sub("SeriesName", "", names(beta_coeff))
knitr::kable(data.frame(nconsumer, unname(beta_coeff)),
             col.names = c("Consumer Series", "$\\beta_1$"),
             caption = "Coefficients for Consumer Model") 

# Commercial
beta_coeff1 <- round(mdl4$finalModel$coefficients[2:12], digits=2)
beta_coeff2 <- round(mdl4$finalModel$coefficients[14:24], digits=2)
ncomm <- sub("SeriesName", "", names(beta_coeff1))
knitr::kable(data.frame(ncomm, unname(beta_coeff1), unname(beta_coeff2)), 
             col.names = c("Commercial Series", "$\\beta_1$", "$\\beta_2$"),
             caption = "Coefficients for Commercial Model") 

```

\clearpage

\section{Predictive Model Approaches}\label{app:PredModel}

\subsection{Linear Model}

\subsection{Proportional Odds Model}

\subsection{Time Series Analysis}

Considering the pNPS score as a stochastic process where the random variables are realizations of pNPS value, attempts to perform classical  time-series analysis.  If $X_t$ denoted pNPS at time $t$, the auto-regressive moving-average(ARMA) model \cite{box2015time} would be

$$X_{t} = \sum_{i = 1}^p \alpha_i X_{t-i} + \sum_{j = 1}^q  \beta_j \theta_{t-j} $$.

where $\alpha_i$ $\forall i \in \{1,2,\ldots, p\}$  and $\beta_j$ $\forall j \in \{1,2,\ldots, q\}$ are coefficients of the linear model making a 1-step prediction of pNPS,  $\theta_t$ is the moving average component at time $t$ following a normal distribution with zero mean and variance $\sigma^2_w$. As the pNPS scores are heteroscedastic, the time series are also differenced $d$ times to satisfy the condition of stationarity.  Note that $p$, $d$ and $q$ are  referred to as the order of the model.  In Tables \ref{tab:ARIMA_Commercial} and \ref{tab:ARIMA_Consumer}, we present train set errors of best ARIMA model with the lowest Akaike information criterion (AIC) with order $(p,q,r)$.  The pNPS calcualtions were made over a period of seven days.  It is also observed that temporal correlation is sensitive to the series.

```{r}
series.ts.main.com <- series.ts.main.com%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0,SeriesName != "ACCESSORIES", SeriesName != "ANNIVERSARY")
series.ts.main <- series.ts.main%>% mutate(MAE = round(MAE,digits = 2),RMSE = round(RMSE,digits = 2)) %>%
  filter(MAE != 0, SeriesName != "SMART HOME", SeriesName != "OTHER")
knitr::kable(series.ts.main.com[series.ts.main.com$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Commercial", caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))
knitr::kable(series.ts.main[series.ts.main$Duration == 7,-5],row.names = FALSE,label = "tab:ARIMA_Consumer",caption = "Series wise Consumer Survey", col.names = c("Series", "Order", "MAE", "RMSE"))


```


While the intention is analyze the statistical relationship between adjacent samples of pNPS calculations, the epoch overwhich the pNPS scores are varied from 1 to 30 days.  Figures \ref{fig:arima.comm} and \ref{fig:arima.con} shows the Mean Absolute Errors as a funtion of the epoch over which the pNPS score was calculated.  It is observed for a majority of the Series, the surveys are least correlated with a two-week window.

```{r figs, echo = FALSE,  fig.cap = "\\label{fig:arima.comm} Train set pNPS prediction errors vs window size for commercial data.", fig.width = 8}
plt.com <- ggplot(series.ts.main.com, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.com
```

```{r figs_con, echo = FALSE, fig.cap="\\label{fig:arima.con} Train set pNPS prediction errors vs window size for consumer data.", fig.width = 8}
plt.con <- ggplot(series.ts.main, aes(x = Duration, y = MAE, color = SeriesName)) + geom_line(size = 1)+xlab("Duration of pNPS window (Days)")+theme(legend.position = "bottom")+labs(color = "")+guides(color = guide_legend(nrow = 4))
plt.con
```


\section{General}

## Category wise correlation between pNPS and PSI 

```{r eval=TRUE, fig.cap = "\\label{fig:neg-corr} Correlation between pNPS and PSI for Customer Series", fig.width = 8}
# slice.low %>% ggplot(aes(x = psi, y = nps, group = SeriesName))+geom_point()+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)

slice.high %>% ggplot(aes(x = psi, y = nps, group = SeriesName))+geom_point()+theme(legend.position = "none")+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)+xlab("PSI")+ylab("pNPS")


```

```{r eval=TRUE, fig.cap = "\\label{fig:neg-corr2} Correlation between pNPS and PSI for Commercial Series", fig.width = 8}
# slice.low %>% ggplot(aes(x = psi, y = nps, group = SeriesName))+geom_point()+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)

slice.high2 %>% ggplot(aes(x = psi, y = nps, group = SeriesName))+geom_point()+theme(legend.position = "none")+geom_smooth(method="lm", se=FALSE, color="darkgrey")+facet_wrap(.~Category)+xlab("PSI")+ylab("pNPS")


```


## Summary of correlations

```{r, eval=TRUE}
high <- summary.cat %>% filter(correlation >0.3) %>% group_by(SeriesName) %>% summarise(rel.high = n())
low <- summary.cat %>% filter(correlation < (-0.3)) %>% group_by(SeriesName) %>% summarise(rel.low = n())
summary = merge(high, low, by = "SeriesName")
# dcast(slice.high[,c(1,3)], formula = Category~SeriesName)
# high.cor.list <-  as.dict high.corr[,c(-3)]# dcast(,formula = SeriesName~Category)

knitr::kable(summary,  caption = "\\label{tab:corr_category} Category wise Consumer Survey-Sentiment correlation ($\\rho$)",  row.names = FALSE,col.names = c("Series", "$\\rho\\ge 0.3$", "$\\rho$ < -0.3"))
```

```{r, eval=TRUE}
high <- summary.cat %>% filter(correlation >0.3) %>% group_by(SeriesName) %>% summarise(rel.high = n())
low <- summary.cat %>% filter(correlation < (-0.3)) %>% group_by(SeriesName) %>% summarise(rel.low = n())
summary = merge(high, low, by = "SeriesName")
# dcast(slice.high[,c(1,3)], formula = Category~SeriesName)
# high.cor.list <-  as.dict high.corr[,c(-3)]# dcast(,formula = SeriesName~Category)

knitr::kable(summary,  caption = "\\label{tab:corr_category2} Category wise Commercial Survey-Sentiment correlation ($\\rho$)",  row.names = FALSE,col.names = c("Series", "$\\rho\\ge 0.3$", "$\\rho$ < -0.3"))
```

## All Categories with negative PSI 

```{r}
knitr::kable(neg.psi,col.names = c("Category","Series", "$\\rho$","PSI"), caption = "\\label{tab:neg.psi} Categories the significantly correlate with NPS scores for Consumer Series ($\\rho$)",  row.names = FALSE)
```

```{r}
knitr::kable(neg.psi2,col.names = c("Category","Series", "$\\rho$","PSI"), caption = "\\label{tab:neg.psi2} Categories that significantly correlate with NPS scores for Commercial Series($\\rho$)",  row.names = FALSE)
```

\clearpage

\section{MDP Source Code}\label{app:MDP}

\lstinputlisting{../LenovoAnalysis/MDP_final.R}

\clearpage

\section{Additional R code}

\subsection{data\_prep.R}

\lstinputlisting{../LenovoAnalysis/data_prep.R}

\subsection{helper\_calcs.R}

\lstinputlisting{../LenovoAnalysis/helper_calcs.R}
